[
  {
    "objectID": "takehome/takehome3new.html",
    "href": "takehome/takehome3new.html",
    "title": "takehome3",
    "section": "",
    "text": "In this VAST Challenge project, we analyze the impact of SouthSeafood Express Corp’s illegal fishing on the business network. The first question explores how the network and competing businesses change due to the incident. The second question identifies which companies benefited from SouthSeafood Express Corp’s legal troubles."
  },
  {
    "objectID": "takehome/takehome3new.html#getting-started",
    "href": "takehome/takehome3new.html#getting-started",
    "title": "takehome3",
    "section": "Getting Started",
    "text": "Getting Started\n\n\nCode\npacman::p_load(tidytext, readtext, quanteda, tidyverse, jsonlite, igraph, tidygraph, ggraph, visNetwork, clock, graphlayouts,ggplot2)"
  },
  {
    "objectID": "takehome/takehome3new.html#importing-json-file",
    "href": "takehome/takehome3new.html#importing-json-file",
    "title": "takehome3",
    "section": "Importing JSON File",
    "text": "Importing JSON File\nDirect import of the mc3.json file shows an error message indicating that there’s an invalid character in the JSON text, specifically “NaN”. As “NaN” is not recognised as a valid value, preprocessing of the JSON file to replace “NaN” is required.\n\n\nCode\n# Read the JSON file as text\njson_text &lt;- readLines(\"data/mc3.json\" ,warn = FALSE)\n\n# Replace \"NaN\" with \"null\"\njson_text_fixed &lt;- gsub(\"NaN\", \"null\", json_text)\n\n# Write the fixed JSON text back to a file\nwriteLines(json_text_fixed, \"data/mc3_fixed.json\")\n\n\nImporting preprocessed mc3_fixed.json file\n\n\nCode\nmc3_data &lt;- fromJSON(\"data/mc3_fixed.json\")"
  },
  {
    "objectID": "takehome/takehome3new.html#data-cleaning",
    "href": "takehome/takehome3new.html#data-cleaning",
    "title": "takehome3",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nMissing Values\nIdentify the percentage of missing values within the dataset\n\n\nCode\n# Function to calculate missing value percentages\ncalculate_missing_percentage &lt;- function(df) {\n  total_values &lt;- nrow(df) * ncol(df)\n  missing_values &lt;- sum(is.na(df))\n  missing_percentage &lt;- (missing_values / total_values) * 100\n  return(missing_percentage)\n}\n\n\nMissing percentage of nodes\n\n\nCode\nnodes_missing_percentage &lt;- calculate_missing_percentage(mc3_data[[\"nodes\"]])\nnodes_missing_percentage\n\n\n[1] 35.11952\n\n\n\n\nCode\nnodes_missing_by_column &lt;- sapply(mc3_data[[\"nodes\"]], function(x) sum(is.na(x)) / length(x) * 100)\n\n\nMissing percentage of edges\n\n\nCode\nlinks_missing_percentage &lt;- calculate_missing_percentage(mc3_data[[\"links\"]])\nlinks_missing_percentage\n\n\n[1] 9.059973\n\n\nCode\nlinks_missing_by_column &lt;- sapply(mc3_data[[\"links\"]], function(x) sum(is.na(x)) / length(x) * 100)\nlinks_missing_by_column\n\n\n       start_date              type   _last_edited_by _last_edited_date \n        0.1187069         0.0000000         0.0000000         0.0000000 \n      _date_added       _raw_source        _algorithm            source \n        0.0000000         0.0000000         0.0000000         0.0000000 \n           target               key          end_date \n        0.0000000         0.0000000        99.5410000 \n\n\nPrint missing percentages\n\n\nCode\n# \nprint(nodes_missing_percentage)\n\n\n[1] 35.11952\n\n\nCode\nprint(nodes_missing_by_column)\n\n\n             type           country   ProductServices    PointOfContact \n          0.00000           0.00000          85.34204          85.38334 \n        HeadOfOrg     founding_date           revenue  TradeDescription \n         85.35691          85.34204          85.36847          85.34204 \n  _last_edited_by _last_edited_date       _date_added       _raw_source \n          0.00000           0.00000           0.00000           0.00000 \n       _algorithm                id               dob \n          0.00000           0.00000          14.65796 \n\n\nCode\nprint(links_missing_percentage)\n\n\n[1] 9.059973\n\n\nCode\nprint(links_missing_by_column)\n\n\n       start_date              type   _last_edited_by _last_edited_date \n        0.1187069         0.0000000         0.0000000         0.0000000 \n      _date_added       _raw_source        _algorithm            source \n        0.0000000         0.0000000         0.0000000         0.0000000 \n           target               key          end_date \n        0.0000000         0.0000000        99.5410000 \n\n\n\nObservations:Actions:\n\n\n\nNodes Data:\n\nProductServices, PointOfContact, HeadOfOrg, founding_date, revenue, and TradeDescription columns have a high percentage of missing values (around 85%).\nThe dob column has about 14.7% missing values.\nOther columns (type, country, _last_edited_by, _last_edited_date, _date_added, _raw_source, _algorithm, and id) have no missing values.\n\nLinks Data:\n\nend_date has a very high percentage of missing values (around 99.5%).\n\n\n\n\n\nFilled missing values in HeadOfOrg with “Unknown”.\nFilled missing values in revenue with 0.\nFilled missing values in start_date and end_date with “Unknown”.\n\n\n\n\nHandle missing values\n\n\nCode\n# Select crucial columns and fill missing values where appropriate\ncleaned_nodes &lt;- mc3_data[[\"nodes\"]] %&gt;%\n  select(id, type, country, HeadOfOrg, revenue,ProductServices,PointOfContact,founding_date,TradeDescription,dob,\n         `_last_edited_by`, `_last_edited_date`, `_date_added`, `_raw_source`, `_algorithm`) %&gt;%\n  mutate(HeadOfOrg = ifelse(is.na(HeadOfOrg), \"Unknown\", HeadOfOrg),\n         revenue = ifelse(is.na(revenue), 0, revenue))\n\n# Handle missing values in links\n# Select crucial columns and fill missing values where appropriate\ncleaned_links &lt;- mc3_data[[\"links\"]] %&gt;%\n  select(key,source, target, type, start_date, end_date, `_last_edited_by`, `_last_edited_date`, `_date_added`, `_raw_source`, `_algorithm`) %&gt;%\n  mutate(start_date = ifelse(is.na(start_date), \"Unknown\", start_date),\n         end_date = ifelse(is.na(end_date), \"Unknown\", end_date))\n\n# Ensure proper data types\ncleaned_nodes &lt;- cleaned_nodes %&gt;%\n  mutate(\n    id = as.character(id),\n    type = as.character(type),\n    country = as.character(country),\n    HeadOfOrg = as.character(HeadOfOrg),\n    revenue = as.numeric(revenue),\n    `_last_edited_by` = as.character(`_last_edited_by`),\n    `_last_edited_date` = as.character(`_last_edited_date`),\n    `_date_added` = as.character(`_date_added`),\n    `_raw_source` = as.character(`_raw_source`),\n    `_algorithm` = as.character(`_algorithm`)\n  )\n\ncleaned_links &lt;- cleaned_links %&gt;%\n mutate(\n    source = as.character(source),\n    target = as.character(target),\n    type = as.character(type),\n    start_date = as.character(start_date),\n    end_date = as.character(end_date),\n    `_last_edited_by` = as.character(`_last_edited_by`),\n    `_last_edited_date` = as.character(`_last_edited_date`),\n    `_date_added` = as.character(`_date_added`),\n    `_raw_source` = as.character(`_raw_source`),\n    `_algorithm` = as.character(`_algorithm`)\n  )\n\n\n\n\nCheck for data types\n\n\nCode\n# Ensure correct data types for nodes\ncleaned_nodes &lt;- cleaned_nodes %&gt;%\n  mutate(\n    id = as.character(id),\n    type = as.character(type),\n    country = as.character(country),\n    HeadOfOrg = as.character(HeadOfOrg),\n    revenue = as.numeric(revenue),\n      dob = as.POSIXct(dob, format=\"%Y-%m-%dT%H:%M:%S\"),\n    `_last_edited_by` = as.character(`_last_edited_by`),\n    `_last_edited_date` = as.POSIXct(`_last_edited_date`, format=\"%Y-%m-%dT%H:%M:%S\"),\n    founding_date=as.POSIXct(founding_date, format=\"%Y-%m-%dT%H:%M:%S\"),\n    `_date_added` = as.POSIXct(`_date_added`, format=\"%Y-%m-%dT%H:%M:%S\"),\n    `_raw_source` = as.character(`_raw_source`),\n    `_algorithm` = as.character(`_algorithm`)\n    \n  )\n\n# Ensure correct data types for links\ncleaned_links &lt;- cleaned_links %&gt;%\n mutate(\n    source = as.character(source),\n    target = as.character(target),\n    type = as.character(type),\n    start_date = as.POSIXct(start_date, format=\"%Y-%m-%dT%H:%M:%S\"),\n    end_date = as.POSIXct(end_date, format=\"%Y-%m-%dT%H:%M:%S\"),\n    `_last_edited_by` = as.character(`_last_edited_by`),\n    `_last_edited_date` = as.POSIXct(`_last_edited_date`, format=\"%Y-%m-%dT%H:%M:%S\"),\n    `_date_added` = as.POSIXct(`_date_added`, format=\"%Y-%m-%dT%H:%M:%S\"),\n    `_raw_source` = as.character(`_raw_source`),\n    `_algorithm` = as.character(`_algorithm`)\n  )\n\n# Print cleaned data for inspection\nglimpse(cleaned_nodes)\n\n\nRows: 60,520\nColumns: 15\n$ id                  &lt;chr&gt; \"Abbott, Mcbride and Edwards\", \"Abbott-Gomez\", \"Ab…\n$ type                &lt;chr&gt; \"Entity.Organization.Company\", \"Entity.Organizatio…\n$ country             &lt;chr&gt; \"Uziland\", \"Mawalara\", \"Uzifrica\", \"Islavaragon\", …\n$ HeadOfOrg           &lt;chr&gt; \"Émilie-Susan Benoit\", \"Honoré Lemoine\", \"Jules La…\n$ revenue             &lt;dbl&gt; 5994.73, 71766.67, 0.00, 0.00, 4746.67, 46566.67, …\n$ ProductServices     &lt;chr&gt; \"Unknown\", \"Furniture and home accessories\", \"Food…\n$ PointOfContact      &lt;chr&gt; \"Rebecca Lewis\", \"Michael Lopez\", \"Steven Robertso…\n$ founding_date       &lt;dttm&gt; 1954-04-24, 2009-06-12, 2029-12-15, 1972-02-16, 1…\n$ TradeDescription    &lt;chr&gt; \"Unknown\", \"Abbott-Gomez is a leading manufacturer…\n$ dob                 &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `_last_edited_by`   &lt;chr&gt; \"Pelagia Alethea Mordoch\", \"Pelagia Alethea Mordoc…\n$ `_last_edited_date` &lt;dttm&gt; 2035-01-01, 2035-01-01, 2035-01-01, 2035-01-01, 2…\n$ `_date_added`       &lt;dttm&gt; 2035-01-01, 2035-01-01, 2035-01-01, 2035-01-01, 2…\n$ `_raw_source`       &lt;chr&gt; \"Existing Corporate Structure Data\", \"Existing Cor…\n$ `_algorithm`        &lt;chr&gt; \"Automatic Import\", \"Automatic Import\", \"Automatic…\n\n\nCode\nglimpse(cleaned_links)\n\n\nRows: 75,817\nColumns: 11\n$ key                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ source              &lt;chr&gt; \"Avery Inc\", \"Berger-Hayes\", \"Bowers Group\", \"Bowm…\n$ target              &lt;chr&gt; \"Allen, Nichols and Thompson\", \"Jensen, Morris and…\n$ type                &lt;chr&gt; \"Event.Owns.Shareholdership\", \"Event.Owns.Sharehol…\n$ start_date          &lt;dttm&gt; 2016-10-29, 2035-06-03, 2028-11-20, 2024-09-04, 2…\n$ end_date            &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `_last_edited_by`   &lt;chr&gt; \"Pelagia Alethea Mordoch\", \"Niklaus Oberon\", \"Pela…\n$ `_last_edited_date` &lt;dttm&gt; 2035-01-01, 2035-07-15, 2035-01-01, 2035-01-01, 2…\n$ `_date_added`       &lt;dttm&gt; 2035-01-01, 2035-07-15, 2035-01-01, 2035-01-01, 2…\n$ `_raw_source`       &lt;chr&gt; \"Existing Corporate Structure Data\", \"Oceanus Corp…\n$ `_algorithm`        &lt;chr&gt; \"Automatic Import\", \"Manual Entry\", \"Automatic Imp…\n\n\n\n\nChanging field name\n\n\nCode\ncleaned_nodes &lt;- cleaned_nodes %&gt;%\n  rename(\"last_edited_by\" = \"_last_edited_by\",\n         \"date_added\" = \"_date_added\",\n         \"last_edited_date\" = \"_last_edited_date\",\n         \"raw_source\" = \"_raw_source\",\n         \"algorithm\" = \"_algorithm\") \n\ncleaned_links&lt;- cleaned_links %&gt;%\n  rename(\"last_edited_by\" = \"_last_edited_by\",\n         \"date_added\" = \"_date_added\",\n         \"last_edited_date\" = \"_last_edited_date\",\n         \"raw_source\" = \"_raw_source\",\n         \"algorithm\" = \"_algorithm\") \n\n\n\n\nSplit ‘type’ column into separate columns\nWe are going to tidy the type column by creating two columns “entity2,entity3”.\n\n\nCode\nword_list1 &lt;- strsplit(cleaned_nodes$type, \"\\\\.\")\nmax_elements1 &lt;- max(lengths(word_list1))\nword_list_padded1 &lt;- lapply(word_list1, \nfunction(x) c(x, rep(NA, max_elements1 - length(x))))\nword_df1 &lt;- do.call(rbind, word_list_padded1)\ncolnames(word_df1) &lt;- paste0(\"entity\", 1:max_elements1)\nword_df1 &lt;- as_tibble(word_df1) %&gt;%\n  select(entity2, entity3)\nclass(word_df1)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThe steps below will be used to split text in type column into two columns\n\n\nCode\nword_list &lt;- strsplit(cleaned_links$type, \"\\\\.\")\nmax_elements &lt;- max(lengths(word_list))\nword_list_padded &lt;- lapply(word_list, \nfunction(x) c(x, rep(NA, max_elements - length(x))))\nword_df &lt;- do.call(rbind, word_list_padded)\ncolnames(word_df) &lt;- paste0(\"event\", 1:max_elements)\nword_df &lt;- as_tibble(word_df) %&gt;%\n  select(event2, event3)\nclass(word_df)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nSince the output above is a matrix, the code chunk above is used to convert word_df into a tibble data.frame.\n\n\nCode\ncleaned_nodes &lt;- cleaned_nodes %&gt;%\n  cbind(word_df1)\n\n\n\n\nCode\ncleaned_links &lt;- cleaned_links %&gt;%\n  cbind(word_df)\n\n\nThe code chunk above appends the extracted columns back to edges tibble data.frame.\n\n\nCode\nwrite_rds(cleaned_nodes, \"data/rds/cleaned_nodes.rds\")\nwrite_rds(cleaned_links, \"data/rds/cleaned_links.rds\")\n\n\nabove code write into R rds file format."
  },
  {
    "objectID": "takehome/takehome3new.html#question-3",
    "href": "takehome/takehome3new.html#question-3",
    "title": "takehome3",
    "section": "Question 3",
    "text": "Question 3\nBy analyzing the ownership structure, we tracked changes in most influential individuals (VIP) networks over time, identifying key individuals with increasing influence.\n\nPart 1: Data Wrangling\nSplit the nodes into people and companies, and filter ownership-related edges\n\n\nCode\n# Split the nodes into people and companies\nnodes_people &lt;- cleaned_nodes %&gt;% filter(entity2 == \"Person\")\nnodes_company &lt;- cleaned_nodes %&gt;% filter(entity2 == \"Organization\")\n\n\n\n\nCode\n# Filter the links to include only ownership-related edges\nlinks_owns &lt;- cleaned_links %&gt;% filter(event2 == \"Owns\")\n\n\n\n\nCode\nnodes_people &lt;- nodes_people %&gt;%\n  rowwise() %&gt;%\n  mutate('no_owns' = sum(links_owns$source == id))\n\nnodes_people$no_owns &lt;- as.numeric(nodes_people$no_owns)\n\n\n\n\nCode\n# Calculate the unique counts of 'no_owns' and their corresponding counts and percentages\nowns_summary &lt;- nodes_people %&gt;%\n  group_by(no_owns) %&gt;%\n  summarise(count = n()) %&gt;%\n  mutate(percentage = (count / sum(count)) * 100)\n\n# Display the summary\nprint(owns_summary)\n\n\n# A tibble: 19 × 3\n   no_owns count percentage\n     &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;\n 1       0   147    0.285  \n 2       1 46370   89.8    \n 3       2  4032    7.81   \n 4       3   665    1.29   \n 5       4   245    0.474  \n 6       5    80    0.155  \n 7       6    34    0.0658 \n 8       7    21    0.0407 \n 9       8    11    0.0213 \n10       9     7    0.0136 \n11      10     2    0.00387\n12      11     4    0.00774\n13      12     3    0.00581\n14      13     2    0.00387\n15      15     1    0.00194\n16      18     2    0.00387\n17      29     1    0.00194\n18      91    18    0.0349 \n19      92     4    0.00774\n\n\nTo define and identify influential people based on an ownership threshold. It filters the nodes to keep only those with a significant number of ownerships\n\n\nCode\n# Define the threshold for 'influential'\nvip_threshold &lt;- 91\n\n# Filter to keep only influential people and select relevant columns\nvip &lt;- nodes_people %&gt;%\n  filter(no_owns &gt;= vip_threshold) %&gt;%\n  select(id, country, dob, last_edited_date, date_added, no_owns)\n\n# Display the updated vip data frame\nglimpse(vip)\n\n\nRows: 22\nColumns: 6\nRowwise: \n$ id               &lt;chr&gt; \"Kelsey Ortega\", \"Joseph Gentry\", \"Cynthia Anderson\",…\n$ country          &lt;chr&gt; \"n.a.\", \"n.a.\", \"n.a.\", \"n.a.\", \"n.a.\", \"n.a.\", \"n.a.…\n$ dob              &lt;dttm&gt; 1974-11-26, 1980-11-08, 1991-07-23, 2013-10-03, 1981…\n$ last_edited_date &lt;dttm&gt; 2035-01-01, 2035-01-01, 2035-01-01, 2035-01-01, 2035…\n$ date_added       &lt;dttm&gt; 2035-01-01, 2035-01-01, 2035-01-01, 2035-01-01, 2035…\n$ no_owns          &lt;dbl&gt; 91, 91, 91, 92, 91, 92, 91, 91, 91, 91, 91, 91, 91, 9…\n\n\nfilter the ownership connections to include only those involving these influential individuals\n\n\nCode\n# Filter links_owns to keep only those connections where the source is in the vip list\nvip_connections &lt;- links_owns %&gt;%\n  filter(source %in% vip$id)%&gt;%\n  select(source, target,start_date,end_date,last_edited_date, date_added)\n\n# Display the updated vip_connections data frame\nglimpse(vip_connections)\n\n\nRows: 2,006\nColumns: 6\n$ source           &lt;chr&gt; \"Kelsey Ortega\", \"Kelsey Ortega\", \"Kelsey Ortega\", \"K…\n$ target           &lt;chr&gt; \"Mitchell-Glover\", \"Anderson, Smith and Weber\", \"Orr …\n$ start_date       &lt;dttm&gt; 2017-08-11, 2028-12-13, 2016-09-18, 2034-12-16, 2032…\n$ end_date         &lt;dttm&gt; NA, NA, NA, NA, 2035-07-13, NA, NA, NA, NA, NA, NA, …\n$ last_edited_date &lt;dttm&gt; 2035-01-01, 2035-01-01, 2035-01-01, 2035-01-01, 2035…\n$ date_added       &lt;dttm&gt; 2035-01-01, 2035-01-01, 2035-01-01, 2035-01-01, 2035…\n\n\n\n\nPart 1: Network Graph\nFinally, plot the network, highlighting the structure and connections of influential individuals.\n\n\nCode\n# Create graph from VIP connections\ng_vip &lt;- graph_from_data_frame(d = vip_connections, directed = TRUE)\n\n# Identify VIPs and Companies\nV(g_vip)$type &lt;- ifelse(V(g_vip)$name %in% nodes_people$id, \"VIP\", \"Company\")\n\n# Define colors and sizes\nV(g_vip)$color &lt;- ifelse(V(g_vip)$type == \"VIP\", \"blue\", \"orange\")\nV(g_vip)$size &lt;- ifelse(V(g_vip)$type == \"VIP\", 8, 5)\n\n# Plot the network\nplot(g_vip, vertex.label = NA, vertex.size = V(g_vip)$size, edge.arrow.size = 0.5, \n     vertex.color = V(g_vip)$color, main = \"VIP Connections Network\")\n\n\n\n\n\n\n\n\n\nThe plot represents the VIP Connections Network, with blue nodes indicating influential VIPs and orange nodes representing companies they own. Directed edges illustrate ownership, pointing from VIPs to companies. This visualization highlights the dense centrality of VIPs, showcasing their extensive control across multiple companies. By examining these connections, we can infer the structure and extent of VIP influence within the network and help FishEye identify influential individuals within the business network, highlighting ownership structures and central figures. By tracking ownership changes over time, FishEye can pinpoint who controls companies involved in illegal fishing activities.\nWhile this plot provides a static snapshot, in the following we shall create similar plots for different time periods can reveal changes in ownership and influence over time.\n\n\nPart 2: Temporal Analysis\nAggregate Ownership Changes by Year\n\n\nCode\nchange_over_time1 &lt;- links_owns %&gt;%\n  group_by(start_date) %&gt;%\n  summarize(count = n()) %&gt;%\n  drop_na()\n\nlinks_owns&lt;- links_owns %&gt;%\n  mutate(start_year = format(start_date, \"%Y\"))\n\n# Aggregate ownership changes by year\nchange_over_time &lt;- links_owns %&gt;%\n  group_by(start_year) %&gt;%\n  summarize(count = n()) %&gt;%\n  drop_na()\n\n\nCreate plots to visualize the changes in ownership over time.\n\n\nCode\n# Plot changes over time\nggplot(change_over_time, aes(x = as.numeric(start_year), y = count)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Changes in Ownership Over Time\",\n       x = \"Year\",\n       y = \"Number of Ownership Changes\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPart 2: Network Graph by Year\nGiven the significant increase in data from 2004 onwards, focusing on every 10 years from 2005 to 2035 would provide a more detailed analysis of changes in ownership and influence.\n\n\nCode\n# Specify the year \nfilter_year &lt;- 2005\n\n# Filter vip_connections by start_year\nvip_connections_filtered &lt;- vip_connections %&gt;%\n  filter(format(start_date, \"%Y\") == filter_year)\n\n# Create the graph object from the filtered vip_connections\ng_vip_filtered &lt;- graph_from_data_frame(d = vip_connections_filtered, directed = TRUE)\n\n# Identify VIPs (nodes_people) and Companies\nV(g_vip_filtered)$type &lt;- ifelse(V(g_vip_filtered)$name %in% nodes_people$id, \"VIP\", \"Company\")\n\n# Define colors and sizes\nV(g_vip_filtered)$color &lt;- ifelse(V(g_vip_filtered)$type == \"VIP\", \"blue\", \"orange\")\nV(g_vip_filtered)$size &lt;- ifelse(V(g_vip_filtered)$type == \"VIP\", 8, 5)\n\n# Plot the network\np2005&lt;-plot(g_vip_filtered, vertex.label = NA, vertex.size = V(g_vip_filtered)$size, edge.arrow.size = 0.5, \n     vertex.color = V(g_vip_filtered)$color, main = paste(\"VIP Connections Network for\", filter_year))\n\n\n\n\n\n\n\n\n\nIn 2005, the network shows a relatively sparse structure with a moderate number of connections. VIPs (blue nodes) are moderately interconnected, indicating a balanced distribution of influence among several key players.\n\n\nCode\n# Specify the year\nfilter_year &lt;- 2015\n\n# Filter vip_connections by start_year\nvip_connections_filtered_2015 &lt;- vip_connections %&gt;%\n  filter(format(start_date, \"%Y\") == filter_year)\n\n# Create the graph object from the filtered vip_connections\ng_vip_filtered_2015 &lt;- graph_from_data_frame(d = vip_connections_filtered_2015, directed = TRUE)\n\n# Identify VIPs (nodes_people) and Companies\nV(g_vip_filtered_2015)$type &lt;- ifelse(V(g_vip_filtered_2015)$name %in% nodes_people$id, \"VIP\", \"Company\")\n\n# Define colors and sizes\nV(g_vip_filtered_2015)$color &lt;- ifelse(V(g_vip_filtered_2015)$type == \"VIP\", \"blue\", \"orange\")\nV(g_vip_filtered_2015)$size &lt;- ifelse(V(g_vip_filtered_2015)$type == \"VIP\", 8, 5)\n\n# Plot the network\np2015 &lt;- plot(g_vip_filtered_2015, vertex.label = NA, vertex.size = V(g_vip_filtered_2015)$size, edge.arrow.size = 0.5, \n     vertex.color = V(g_vip_filtered_2015)$color, main = paste(\"VIP Connections Network for\", filter_year))\n\n\n\n\n\n\n\n\n\nBy 2015, the network has grown denser, suggesting increased interconnectedness and influence consolidation. More VIPs are connected to multiple companies (orange nodes), indicating a significant rise in their influence and control over the network.\n\n\nCode\n# Specify the year\nfilter_year &lt;- 2025\n\n# Filter vip_connections by start_year\nvip_connections_filtered_2025 &lt;- vip_connections %&gt;%\n  filter(format(start_date, \"%Y\") == filter_year)\n\n# Create the graph object from the filtered vip_connections\ng_vip_filtered_2025 &lt;- graph_from_data_frame(d = vip_connections_filtered_2025, directed = TRUE)\n\n# Identify VIPs (nodes_people) and Companies\nV(g_vip_filtered_2025)$type &lt;- ifelse(V(g_vip_filtered_2025)$name %in% nodes_people$id, \"VIP\", \"Company\")\n\n# Define colors and sizes\nV(g_vip_filtered_2025)$color &lt;- ifelse(V(g_vip_filtered_2025)$type == \"VIP\", \"blue\", \"orange\")\nV(g_vip_filtered_2025)$size &lt;- ifelse(V(g_vip_filtered_2025)$type == \"VIP\", 8, 5)\n\n# Plot the network\np2025 &lt;- plot(g_vip_filtered_2025, vertex.label = NA, vertex.size = V(g_vip_filtered_2025)$size, edge.arrow.size = 0.5, \n     vertex.color = V(g_vip_filtered_2025)$color, main = paste(\"VIP Connections Network for\", filter_year))\n\n\n\n\n\n\n\n\n\nThe network continues to expand in 2025, displaying even more complexity and interconnections. This period likely represents a peak in influence for several VIPs, with many of them owning shares in numerous companies, suggesting increased market control.\n\n\nCode\n# Specify the year\nfilter_year &lt;- 2035\n\n# Filter vip_connections by start_year\nvip_connections_filtered_2035 &lt;- vip_connections %&gt;%\n  filter(format(start_date, \"%Y\") == filter_year)\n\n# Create the graph object from the filtered vip_connections\ng_vip_filtered_2035 &lt;- graph_from_data_frame(d = vip_connections_filtered_2035, directed = TRUE)\n\n# Identify VIPs (nodes_people) and Companies\nV(g_vip_filtered_2035)$type &lt;- ifelse(V(g_vip_filtered_2035)$name %in% nodes_people$id, \"VIP\", \"Company\")\n\n# Define colors and sizes\nV(g_vip_filtered_2035)$color &lt;- ifelse(V(g_vip_filtered_2035)$type == \"VIP\", \"blue\", \"orange\")\nV(g_vip_filtered_2035)$size &lt;- ifelse(V(g_vip_filtered_2035)$type == \"VIP\", 8, 5)\n\n# Plot the network\np2035 &lt;- plot(g_vip_filtered_2035, vertex.label = NA, vertex.size = V(g_vip_filtered_2035)$size, edge.arrow.size = 0.5, \n     vertex.color = V(g_vip_filtered_2035)$color, main = paste(\"VIP Connections Network for\", filter_year))\n\n\n\n\n\n\n\n\n\nIn 2035, the network structure shifts to a star-like formation, where a central VIP appears to have gained substantial influence, with direct connections to numerous companies. This indicates a significant consolidation of power and influence, where a few key players dominate the network.\n\n\n\n\n\n\nCaution\n\n\n\nInitially, influence is distributed among several key players, but over the years, it becomes concentrated among fewer individuals, leading to a highly centralized network by 2035. This centralization of power can be both an opportunity for streamlined decision-making and a risk for monopolistic control. Monitoring these changes is crucial for regulatory bodies like FishEye to ensure fair practices and prevent illegal activities within the network."
  },
  {
    "objectID": "takehome/takehome3new.html#question-4",
    "href": "takehome/takehome3new.html#question-4",
    "title": "takehome3",
    "section": "Question 4",
    "text": "Question 4\nFor part 1, the focus was on identifying the network associated with SouthSeafood Express Corp and visualizing how this network and competing businesses changed as a result of their illegal fishing behavior.\n\nPart 1: Identify SouthSeafood Express Corp Node\n\nLocate the node representing SouthSeafood Express Corp in the network.\nCreate a visualization of the network associated with SouthSeafood Express Corp before any changes.\n\n\n\nCode\n# Extract edges connected to SouthSeafood Express Corp\nsouthseafood_edges &lt;- cleaned_links %&gt;%\n  filter(source == \"SouthSeafood Express Corp\" | target == \"SouthSeafood Express Corp\")%&gt;%\n  select(source,target,start_date,end_date,event2)\n\n# Ensure all nodes in the edge list are present in the vertex data frame\nsouthseafood_nodes &lt;- cleaned_nodes %&gt;%\n  filter(id %in% (c(southseafood_edges$source, southseafood_edges$target)))\n\n# Join edges with nodes to ensure all nodes are present\nsouthseafood_edges &lt;- southseafood_edges %&gt;%\n  filter(source %in% southseafood_nodes$id & target %in% southseafood_nodes$id)\n\n# Create graph object for the sub-network\ng_southseafood &lt;- graph_from_data_frame(d = southseafood_edges, vertices = southseafood_nodes, directed = TRUE)\n\n# Visualize the initial network\nplot(g_southseafood, vertex.label = NA, vertex.size = 5, edge.arrow.size = 0.5, \n     vertex.color = \"orange\", main = \"Network Associated with SouthSeafood Express Corp\")\n\n\n\n\n\n\n\n\n\n\n\nPart 1: Identify Competing Businesses\nIdentify and highlight competing businesses within the extracted sub-network.\n\n\nCode\ncompeting_businesses &lt;- cleaned_nodes %&gt;%\n  filter(entity3 == \"FishingCompany\" & id != \"SouthSeafood Express Corp\")\n\n\n\n\nCode\ncompeting_edges &lt;- cleaned_links %&gt;%\n  filter(source %in% competing_businesses$id | target %in% competing_businesses$id) %&gt;%\n  select(source, target, start_date, end_date, event2)\n\n# Combine SouthSeafood Express Corp edges with competing businesses edges\ncombined_edges &lt;- bind_rows(southseafood_edges, competing_edges)\n\n# Extract the combined set of nodes\ncombined_nodes &lt;- cleaned_nodes %&gt;%\n  filter(id %in% c(combined_edges$source, combined_edges$target))\n\n\n\n\nCode\n# Create graph object for the combined network\ng_combined &lt;- graph_from_data_frame(d = combined_edges, vertices = combined_nodes, directed = TRUE)\n\n\n\n\nPart 1: Analyze Temporal Changes based on start_year\n\nFilter the data to show the network before and after the illegal fishing incident(assume the incident happened in 2023)\nCreate visualizations to compare the network structure and connections before and after the incident.\n\n\n\nCode\n# Assume the accident happened in 2023\nincident_year &lt;- 2023\n\n# Filter edges before the incident\nedges_before &lt;- combined_edges %&gt;%\n  filter(format(start_date, \"%Y\") &lt; incident_year)\n\n# Filter edges after the incident\nedges_after &lt;- combined_edges %&gt;%\n  filter(format(start_date, \"%Y\") &gt;= incident_year)\n\n# Create graph objects for before and after the incident\ng_before &lt;- graph_from_data_frame(d = edges_before, vertices = combined_nodes, directed = TRUE)\ng_after &lt;- graph_from_data_frame(d = edges_after, vertices = combined_nodes, directed = TRUE)\n\n\n\n\nPart 1: Visualize the Temporal Changes\nIdentify and highlight significant changes in connections and structure due to the illegal fishing behavior and subsequent closure.\n\n\nCode\npar(mfrow = c(2, 1))\n\nplot_before &lt;- ggraph(g_before, layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = 0.8), show.legend = FALSE, color = \"gray\", width = 1) +\n  geom_node_point(aes(color = ifelse(name == \"SouthSeafood Express Corp\", \"SouthSeafood\", \n                                     ifelse(type == \"Entity.Organization.FishingCompany\", \"FishingCompany\", \"Other\"))), \n                  size = 3, alpha = 0.6, show.legend = TRUE) + # Adjusted alpha for transparency\n  scale_color_manual(values = c(\"SouthSeafood\" = \"red\", \"FishingCompany\" = \"blue\", \"Other\" = \"orange\"),\n                     name = \"Type\") + # Shortened legend title\n  theme_void() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Network Before Incident\")\n\n# Show the plot for the network before the incident\nplot_before\n\n\n\n\n\n\n\n\n\nCode\nplot_after &lt;- ggraph(g_after, layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = 0.8), show.legend = FALSE, color = \"gray\", width = 1) +\n  geom_node_point(aes(color = ifelse(name == \"SouthSeafood Express Corp\", \"SouthSeafood\", \n                                     ifelse(type == \"Entity.Organization.FishingCompany\", \"FishingCompany\", \"Other\"))), \n                  size = 3, alpha = 0.6, show.legend = TRUE) + # Adjusted alpha for transparency\n  scale_color_manual(values = c(\"SouthSeafood\" = \"red\", \"FishingCompany\" = \"blue\", \"Other\" = \"orange\"),\n                     name = \"Type\") + # Shortened legend title\n  theme_void() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Network After Incident\")\n\n# Show the plot for the network after the incident\nplot_after\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nThe number of blue nodes (fishing companies) appears to have decreased.\nSouthSeafood Express Corp (red node) remains central but its connections might have changed, indicating possible impact from the incident.\n\n\n\nFor part 2, since we cannot use revenue data over time, we will focus on identifying which companies potentially benefited from SouthSeafood Express Corp’s legal troubles by analyzing changes in network centrality measures.\n\n\nPart 2: Calculate Centrality Measures Before and After the Incident\n\n\nCode\n# Calculate degree centrality before the incident\ndegree_before &lt;- degree(g_before, mode = \"all\")\n\n# Calculate degree centrality after the incident\ndegree_after &lt;- degree(g_after, mode = \"all\")\n\n# Combine degree centrality measures into a data frame\ncentrality_change &lt;- data.frame(\n  id = names(degree_before),\n  degree_before = degree_before,\n  degree_after = degree_after\n)\n\n# Calculate the change in degree centrality\ncentrality_change &lt;- centrality_change %&gt;%\n  mutate(change = degree_after - degree_before)\n\n# Display companies with the most positive change in degree centrality\ntop_beneficiaries &lt;- centrality_change %&gt;%\n  arrange(desc(change)) %&gt;%\n  head(10)\n\nprint(top_beneficiaries)\n\n\n                                           id degree_before degree_after change\nAnderson-Roberts             Anderson-Roberts             0           36     36\nHall, Hartman and Hall Hall, Hartman and Hall             0           30     30\nKirk Inc                             Kirk Inc             0           18     18\nWatson-Gray                       Watson-Gray             0           18     18\nParker Inc                         Parker Inc             0           17     17\nMullins-Carrillo             Mullins-Carrillo             0           15     15\nTorres, Ross and Brown Torres, Ross and Brown             0           14     14\nByrd and Sons                   Byrd and Sons             0           13     13\nHaynes-Lucero                   Haynes-Lucero             0           13     13\nLutz-Fleming                     Lutz-Fleming             0           13     13\n\n\n\n\nPart 2: Determine Entity Type\n\n\nCode\n# Merge with cleaned_nodes to get the entity type\ntop_beneficiaries_info &lt;- top_beneficiaries %&gt;%\n  left_join(cleaned_nodes, by = c(\"id\" = \"id\")) %&gt;%\n  select(id, change,entity3)\n\n# Display the entity type of top beneficiaries\nprint(top_beneficiaries_info)\n\n\n                       id change        entity3\n1        Anderson-Roberts     36 FishingCompany\n2  Hall, Hartman and Hall     30 FishingCompany\n3                Kirk Inc     18 FishingCompany\n4             Watson-Gray     18 FishingCompany\n5              Parker Inc     17 FishingCompany\n6        Mullins-Carrillo     15 FishingCompany\n7  Torres, Ross and Brown     14 FishingCompany\n8           Byrd and Sons     13 FishingCompany\n9           Haynes-Lucero     13 FishingCompany\n10           Lutz-Fleming     13 FishingCompany\n\n\n\n\nPart 2: Visualize the Changes\n\n\nCode\n# Bar plot of top beneficiaries\nggplot(top_beneficiaries_info, aes(x = reorder(id, change), y = change)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(title = \"Top Beneficiaries by Change in Degree Centrality\",\n       x = \"Company\",\n       y = \"Change in Degree Centrality\",\n       fill = \"Entity Type\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThe results show that the top beneficiaries, all classified as fishing companies, significantly increased their network centrality following SouthSeafood Express Corp’s legal troubles. Anderson-Roberts, Hall, Hartman and Hall, and Kirk Inc., among others, saw the largest gains, suggesting they capitalized on the shift in the network’s structure."
  },
  {
    "objectID": "takehome/takehome1.html",
    "href": "takehome/takehome1.html",
    "title": "Take-home Exercise 1:Singapore Private Residential Market",
    "section": "",
    "text": "Task\nIn this Take Home Exercise, a few compelling and insightful data visualizations for the Singapore private residential market and its sub-markets for the first quarter of 2024 are created using Quarto.\n\n\n\n\n\nflowchart LR\n  A[Data Preparation] --&gt; B[Data Summary]\n  B --&gt; C{Data Cleaning}\n  C --&gt; D[Market Overview Visualization 1: Trend of Average Unit Prices by Planning Region]\n  C --&gt; E[Market Overview Visualization 2: Popularity by Type of Sale]\n\n\n\n\n\n\n\n\nData Preparation\nData Source: private residential property transaction data from 1st January 2023 to 31st March 2024\nLoaded necessary libraries and set the working directory to source transaction data. Aggregated CSV files, collating rows to form a complete dataset. Performed initial data summary, extracting transaction count, date range, unique property types, and regions. Cleaned data, replacing empty values with NAs and converting numeric fields for insightful analysis.\n\n\nCode\npacman::p_load(ggplot2,lubridate,ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse)\n\nsetwd(\"C:/kekekay/ISSS608-VAA/takehome/data\")\nfull_data &lt;-  list.files(\n                    pattern = \"*.csv\",\n                    full.names=T) %&gt;%\n                    lapply(read_csv) %&gt;%\n                    bind_rows()\n\n\n\n\nData Summary\n\n\nCode\n# data summary\ntotal_transactions &lt;- nrow(full_data)\ndate_range &lt;- range(full_data$`Sale Date`, na.rm = TRUE)\nproperty_types &lt;- unique(full_data$`Property Type`)\ntotal_property_types &lt;- length(property_types)\nregion&lt;- unique(full_data$`Planning Region`)\narea&lt;- unique(full_data$`Planning Area`)\n\n\nTotal Transactions:26806\nDate Range is from 01 Apr 2023 to 31 Oct 2023\nTotal Unique Property Types: 6\nList of Property Types: Condominium, Executive Condominium, Terrace House, Semi-Detached House, Apartment, Detached House\n\n\n\nFive regions and planning area boundaries of Singapore were used in this study. Data source: URA (2021).\n\n\nPlanning Region: Central Region, East Region, North Region, North East Region, West Region\nPlanning Area: Bukit Merah, Bedok, Yishun, Sengkang, Hougang, Bukit Timah, Marine Parade, Clementi, Woodlands, Serangoon, Tanglin, Tampines, Kallang, Rochor, Novena, Punggol, Sembawang, Downtown Core, Bishan, Jurong West, Pasir Ris, Queenstown, Bukit Panjang, Bukit Batok, Museum, Newton, Southern Islands, Toa Payoh, Choa Chu Kang, Geylang, River Valley, Orchard, Singapore River, Outram, Tengah, Ang Mo Kio, Jurong East, Mandai, Sungei Kadut, Changi, Paya Lebar\n\n\nData Cleaning\n\n\nCode\ncleaned_data &lt;- full_data %&gt;%\n  mutate(across(c(`Nett Price($)`, `Area (SQM)`, `Unit Price ($ PSM)`), ~replace(., . == \"\" | . == \"-\", NA))) %&gt;%\n  mutate(\n    `Transacted Price ($)` = as.numeric(gsub(\",\", \"\", `Transacted Price ($)`)),\n    `Area (SQFT)` = as.numeric(`Area (SQFT)`),\n    `Unit Price ($ PSF)` = as.numeric(gsub(\",\", \"\", `Unit Price ($ PSF)`)),\n    `Sale Date` = dmy(`Sale Date`),\n    `Area (SQM)` = as.numeric(`Area (SQM)`),\n    `Unit Price ($ PSM)` = as.numeric(gsub(\",\", \"\", `Unit Price ($ PSM)`)),\n    `Nett Price($)` = ifelse(is.na(`Nett Price($)`),\n                             `Area (SQM)` * `Unit Price ($ PSM)`,\n                             as.numeric(gsub(\",\", \"\", `Nett Price($)`)))\n  )\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\n\n\n\nUse of across: The across() function is applied to check and replace empty or placeholder values across specified columns. It replaces any empty strings or ‘-’ with NA.\nCleaning and Converting Data: After the placeholders are handled, the script then cleans up currency and area fields, removing commas and converting them to numeric where necessary.\nConditional Calculation for Nett Price($): After ensuring all data types are correct and placeholders are handled, it calculates Nett Price($) where needed.\n\n\n\n\n\n\nMarket Overview Visualization 1: Trend of Average Unit Prices by Planning Region\n\nCentral RegionEast RegionNorth East RegionNorth RegionWest Region\n\n\n\n\nCode\np1 &lt;- cleaned_data %&gt;%\n  filter(`Planning Region` == \"Central Region\") %&gt;% \n  group_by(Month = floor_date(`Sale Date`, \"month\"), `Type of Sale`, `Property Type`) %&gt;%\n  summarize(Average_Price = mean(`Unit Price ($ PSM)`, na.rm = TRUE), .groups = 'drop') %&gt;%\n  ggplot(aes(x = Month, y = Average_Price, color = `Type of Sale`)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%b %Y\") +\n  labs(\n    title = \"Central Region: Trend of Average Unit Prices Over Time\",\n    x = \"Month\",\n    y = \"Average Unit Price ($ PSM)\"\n  ) +\n  facet_wrap(~ `Property Type`, scales = \"free_y\", strip.position = \"bottom\") +  \n  theme(\n    plot.title = element_text(size = rel(1.5)),\n    legend.position = \"top\",\n    legend.text = element_text(size = rel(0.8)),\n    panel.grid.major = element_line(color = \"grey80\"),\n    panel.grid.minor = element_blank(),\n    plot.margin = margin(10, 10, 10, 10),\n    strip.text = element_text(size = rel(0.8)),  # adjust strip text size\n    axis.text.x = element_text(size = rel(0.8), angle = 45, hjust = 1, vjust = 1),  # adjust x-axis text size\n    axis.ticks.length = unit(-3, \"pt\"),  #aAdjust tick length\n    panel.spacing = unit(1, \"lines\")  # adjust spacing between facets\n  )\n\np1\n\n\n\n\n\n\n\n\n\nIn the Central Region, Q1 2024 presents a stable pricing pattern for apartments, condominiums, and terrace houses, mirroring trends from the previous year. Conversely, detached houses experienced a significant rise in prices, followed by a pronounced dip, particularly within the sub-sale segment, which has now narrowed down to only resale transactions. It shows there was flutuation under Executive condominiums from March to December 2023, culminating in a complete absence of new sales in the subsequent quarter. Meanwhile, semi-detached houses witnessed a singular decline in June 2023, after which prices entered a gradual and steady climb, indicating a stabilizing market as progress through 2024.\n\n\n\n\nCode\np2 &lt;- cleaned_data %&gt;%\n  filter(`Planning Region` == \"East Region\") %&gt;% \n  group_by(Month = floor_date(`Sale Date`, \"month\"), `Type of Sale`, `Property Type`) %&gt;%\n  summarize(Average_Price = mean(`Unit Price ($ PSM)`, na.rm = TRUE), .groups = 'drop') %&gt;%\n  ggplot(aes(x = Month, y = Average_Price, color = `Type of Sale`)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%b %Y\") +\n  labs(\n    title = \"East Region:Trend of Average Unit Prices Over Time\",\n    x = \"Month\",\n    y = \"Average Unit Price ($ PSM)\"\n  ) +\n  facet_wrap(~ `Property Type`, scales = \"free_y\", strip.position = \"bottom\") +  \n  theme(\n    plot.title = element_text(size = rel(1.5)),\n    legend.position = \"top\",\n    legend.text = element_text(size = rel(0.8)),\n    panel.grid.major = element_line(color = \"grey80\"),\n    panel.grid.minor = element_blank(),\n    plot.margin = margin(10, 10, 10, 10),\n    strip.text = element_text(size = rel(0.8)),  \n    axis.text.x = element_text(size = rel(0.8), angle = 45, hjust = 1, vjust = 1),  \n    axis.ticks.length = unit(-3, \"pt\"),  \n    panel.spacing = unit(1, \"lines\")  \n  )\n\np2\n\n\n\n\n\n\n\n\n\nIn East Region, Apartments, condominiums and terrace houses have shown relative price stability, with condominiums displaying a slight uprend. Detached houses have seen erratic price movements with a sharp rise followed by a decline in sub-sale prices. Executive condominiums display notable price swings throughout the year, overall, it still demonstrates an upward price trend. In contrast, semi-detached houses show a significant dip year-end but recover to a gentle upward trend.\n\n\n\n\nCode\np3 &lt;- cleaned_data %&gt;%\n  filter(`Planning Region` == \"North East Region\") %&gt;% \n  group_by(Month = floor_date(`Sale Date`, \"month\"), `Type of Sale`, `Property Type`) %&gt;%\n  summarize(Average_Price = mean(`Unit Price ($ PSM)`, na.rm = TRUE), .groups = 'drop') %&gt;%\n  ggplot(aes(x = Month, y = Average_Price, color = `Type of Sale`)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%b %Y\") +\n  labs(\n    title = \"North East Region: Trend of Average Unit Prices\",\n    x = \"Month\",\n    y = \"Average Unit Price ($ PSM)\"\n  ) +\n  facet_wrap(~ `Property Type`, scales = \"free_y\", strip.position = \"bottom\") +  \n  theme(\n    plot.title = element_text(size = rel(1.5)),\n    legend.position = \"top\",\n    legend.text = element_text(size = rel(0.8)),\n    panel.grid.major = element_line(color = \"grey80\"),\n    panel.grid.minor = element_blank(),\n    plot.margin = margin(10, 10, 10, 10),\n    strip.text = element_text(size = rel(0.8)),  \n    axis.text.x = element_text(size = rel(0.8), angle = 45, hjust = 1, vjust = 1),  \n    axis.ticks.length = unit(-3, \"pt\"), \n    panel.spacing = unit(1, \"lines\")  \n  )\n\np3\n\n\n\n\n\n\n\n\n\nIn the North East region, the fluctuation in the prices of detached houses is quite pronounced. Additionally, there’s a notable upward trend in the prices of executive condominiums, especially within the new sales category.\n\n\n\n\nCode\np4 &lt;- cleaned_data %&gt;%\n  filter(`Planning Region` == \"North Region\") %&gt;% \n  group_by(Month = floor_date(`Sale Date`, \"month\"), `Type of Sale`, `Property Type`) %&gt;%\n  summarize(Average_Price = mean(`Unit Price ($ PSM)`, na.rm = TRUE), .groups = 'drop') %&gt;%\n  ggplot(aes(x = Month, y = Average_Price, color = `Type of Sale`)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%b %Y\") +\n  labs(\n    title = \"North Region:Trend of Average Unit Prices Over Time\",\n    x = \"Month\",\n    y = \"Average Unit Price ($ PSM)\"\n  ) +\n  facet_wrap(~ `Property Type`, scales = \"free_y\", strip.position = \"bottom\") +  \n  theme(\n    plot.title = element_text(size = rel(1.5)),\n    legend.position = \"top\",\n    legend.text = element_text(size = rel(0.8)),\n    panel.grid.major = element_line(color = \"grey80\"),\n    panel.grid.minor = element_blank(),\n    plot.margin = margin(10, 10, 10, 10),\n    strip.text = element_text(size = rel(0.8)),  \n    axis.text.x = element_text(size = rel(0.8), angle = 45, hjust = 1, vjust = 1), \n    axis.ticks.length = unit(-3, \"pt\"),  \n    panel.spacing = unit(1, \"lines\")  \n  )\n\np4\n\n\n\n\n\n\n\n\n\nIn the North Region, apart from condominiums, other property types display considerable instability. However, as of March 2024, apartments, executive condominiums, and terrace houses continue to exhibit an increasing price trend.\n\n\n\n\nCode\np5 &lt;- cleaned_data %&gt;%\n  filter(`Planning Region` == \"West Region\") %&gt;% \n  group_by(Month = floor_date(`Sale Date`, \"month\"), `Type of Sale`, `Property Type`) %&gt;%\n  summarize(Average_Price = mean(`Unit Price ($ PSM)`, na.rm = TRUE), .groups = 'drop') %&gt;%\n  ggplot(aes(x = Month, y = Average_Price, color = `Type of Sale`)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%b %Y\") +\n  labs(\n    title = \"West Region:Trend of Average Unit Prices Over Time\",\n    x = \"Month\",\n    y = \"Average Unit Price ($ PSM)\"\n  ) +\n  facet_wrap(~ `Property Type`, scales = \"free_y\", strip.position = \"bottom\") +  \n  theme(\n    plot.title = element_text(size = rel(1.5)),\n    legend.position = \"top\",\n    legend.text = element_text(size = rel(0.8)),\n    panel.grid.major = element_line(color = \"grey80\"),\n    panel.grid.minor = element_blank(),\n    plot.margin = margin(10, 10, 10, 10),\n    strip.text = element_text(size = rel(0.8)),  \n    axis.text.x = element_text(size = rel(0.8), angle = 45, hjust = 1, vjust = 1), \n    axis.ticks.length = unit(-3, \"pt\"), \n    panel.spacing = unit(1, \"lines\")  \n  )\n\np5\n\n\n\n\n\n\n\n\n\nIn the West Region, December 2023 marked a notable uptick in the prices of new sale apartments, surging from approximately $19,000 PSM to $26,000 PSM, before settling back at $22,000 PSM. Conversely, detached and semi-detached houses demonstrated a downward pricing trend.\n\n\n\nIn summary, the Central Region commands the highest unit prices across the board, while the North Region is distinguished by the lowest. Typically, new sales achieve higher prices than sub-sales and resales. However, an exception is noted in the North East Region, where terrace houses experience the lowest prices in sub-sales. Furthermore, the North East and North Regions are both exhibiting an upward price trend as we progress through Q1 of 2024.\n\n\nMarket Overview Visualization 2: Popularity by Type of Sale\n\nPopularity Overview by Type of SaleNew SaleResaleSub Sale\n\n\n\n\nCode\n# number of transactions by type of sale\ntransactions_by_sale_type &lt;- cleaned_data %&gt;%\n  count(`Type of Sale`) %&gt;%\n  mutate(Percentage = n / sum(n) * 100, \n         Label = paste(`Type of Sale`, round(Percentage, 1), \"%\"))  # set label for each slice\n\n# Custom colors for the pie slices\nslice_colors &lt;- c(\"New Sale\" = \"darkseagreen\", \"Resale\" = \"lavender\", \"Sub Sale\" = \"pink\")\n\n# Create the pie chart\npie_chart &lt;- ggplot(transactions_by_sale_type, aes(x = \"\", y = Percentage, fill = `Type of Sale`)) +\n  geom_col(width = 1) +  # this is to create a bar for each slice with a width that ensures no gaps\n  coord_polar(theta = \"y\") + \n  scale_fill_manual(values = slice_colors) +  \n  geom_text(aes(label = Label), position = position_stack(vjust = 0.5)) +  # add labels of each slice\n  labs(\n    title = \"Popularity Overview by Type of Sale\",\n    x = NULL,\n    y = NULL,\n    fill = \"Type of Sale\"\n  ) +\n  theme_void() +  \n  theme(\n    legend.position = \"bottom\",  # legend postion\n    plot.title = element_text(hjust = 0.5)  # plot title position\n  )\n\npie_chart\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for 'New Sale' transactions\nnew_sale_transactions &lt;- cleaned_data %&gt;%\n  filter(`Type of Sale` == \"New Sale\") %&gt;%\n  count(`Property Type`, `Planning Region`) %&gt;%\n  complete(`Property Type`, `Planning Region`, fill = list(n = 0))  \n\n# heatmap for 'New Sale'\nheatmap_new_sale &lt;- ggplot(new_sale_transactions, aes(x = `Planning Region`, y = `Property Type`, fill = n)) +\n  geom_tile(color = \"white\") +  \n  geom_text(aes(label = n), color = \"black\", size = 3, vjust = 1) +  \n  scale_fill_gradient(low = \"white\", high = \"darkseagreen\", name = \"Transactions\") + \n  labs(\n    title = \"Popularity of New Sale Flats by Planning Region\",\n    x = \"Planning Region\",\n    y = \"Type of Property\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for better readability\n    legend.position = \"right\"  # legend position\n  )\n\nheatmap_new_sale\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for 'Resale' transactions\nnew_sale_transactions &lt;- cleaned_data %&gt;%\n  filter(`Type of Sale` == \"Resale\") %&gt;%\n  count(`Property Type`, `Planning Region`) %&gt;%\n  complete(`Property Type`, `Planning Region`, fill = list(n = 0))  \n\n# Create the heatmap for 'Resale'\nheatmap_Resale &lt;- ggplot(new_sale_transactions, aes(x = `Planning Region`, y = `Property Type`, fill = n)) +\n  geom_tile(color = \"white\") +  \n  geom_text(aes(label = n), color = \"black\", size = 3, vjust = 1) +  \n  scale_fill_gradient(low = \"white\", high = \"lavender\", name = \"Transactions\") +  \n  labs(\n    title = \"Popularity of Resale Flats by Planning Region\",\n    x = \"Planning Region\",\n    y = \"Type of Property\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),  \n    legend.position = \"right\"  \n  )\n\nheatmap_Resale\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter for 'Sub Sale' transactions\nnew_sale_transactions &lt;- cleaned_data %&gt;%\n  filter(`Type of Sale` == \"Sub Sale\") %&gt;%\n  count(`Property Type`, `Planning Region`) %&gt;%\n  complete(`Property Type`, `Planning Region`, fill = list(n = 0)) \n\n# Create the heatmap for 'Sub Sale'\nheatmap_Sub_Sale &lt;- ggplot(new_sale_transactions, aes(x = `Planning Region`, y = `Property Type`, fill = n)) +\n  geom_tile(color = \"white\") +  \n  geom_text(aes(label = n), color = \"black\", size = 3, vjust = 1) + \n  scale_fill_gradient(low = \"white\", high = \"pink\", name = \"Transactions\") +  \n  labs(\n    title = \"Popularity of Sub-sale Flats by Planning Region\",\n    x = \"Planning Region\",\n    y = \"Type of Property\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),  \n    legend.position = \"right\"  \n  )\n\nheatmap_Sub_Sale\n\n\n\n\n\n\n\n\n\n\n\n\nMoving forward, we will explore transaction volumes to gauge popularity. Resale properties dominate the market, accounting for 62% of transactions, followed by new sales at 32.2% and sub-sales at 5.8%. Under both resale and new sale categories, condominiums and apartments in the Central Region are the most favored, while the North Region remains the least preferred. Interestingly, resale transactions show a preference for condominiums, whereas buyers of new sales are inclined towards apartments. In the case of sub-sales, the North East Region emerges as the most popular, with the Central Region trailing behind, yet the North Region consistently ranks as the least favored across all sales types."
  },
  {
    "objectID": "inclass/inclass02/inclass02.html",
    "href": "inclass/inclass02/inclass02.html",
    "title": "In-Class Exercise 2 - Visualising Distribution",
    "section": "",
    "text": "Statistical graphic methods for visualising distribution using ggplot2 and its extensions for:\n\nRidgeline plot\nRaincloud plot"
  },
  {
    "objectID": "inclass/inclass02/inclass02.html#loading-r-packages",
    "href": "inclass/inclass02/inclass02.html#loading-r-packages",
    "title": "In-Class Exercise 2 - Visualising Distribution",
    "section": "Loading R packages",
    "text": "Loading R packages\nNote: Ensure that the pacman package has already been installed.\nThe code chunk below uses p_load() of pacman package to load the tidyverse family of packages.\n\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots,\nggdist, a ggplot2 extension spacially desgin for visualising distribution and uncertainty,\ntidyverse, a family of R packages to meet the modern data science and visual communication needs,\nggthemes, a ggplot extension that provides the user additional themes, scales, and geoms for the ggplots package, and\ncolorspace, an R package provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualisations.\n\n\npacman::p_load(ggdist, ggridges, ggthemes, colorspace, tidyverse)"
  },
  {
    "objectID": "inclass/inclass02/inclass02.html#importing-the-data",
    "href": "inclass/inclass02/inclass02.html#importing-the-data",
    "title": "In-Class Exercise 2 - Visualising Distribution",
    "section": "Importing the Data",
    "text": "Importing the Data\n\nThe code chunk below imports Exam_data.csv into R environment by using read_csv() function of readr package.\nreadr is one of the tidyverse package.\n\n\nexam_df &lt;- read_csv(\"data/Exam_data.csv\")\n\nexam_data contains:\n\nYear end examination grades of a cohort of primary 3 students from a local school.\nThere are a total of seven attributes. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "inclass/inclass02/inclass02.html#varying-fill-colours-along-the-x-axis",
    "href": "inclass/inclass02/inclass02.html#varying-fill-colours-along-the-x-axis",
    "title": "In-Class Exercise 2 - Visualising Distribution",
    "section": "Varying Fill Colours along the X-axis",
    "text": "Varying Fill Colours along the X-axis\nSometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis.\nTo achieve varying fill colours:\n\ngeom_ridgeline_gradient()\ngeom_density_ridges_gradient()\n\nHowever, they do not allow for alpha transparency.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_df, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [°C]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English Grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()"
  },
  {
    "objectID": "inclass/inclass02/inclass02.html#mapping-probabilities-directly-onto-colour",
    "href": "inclass/inclass02/inclass02.html#mapping-probabilities-directly-onto-colour",
    "title": "In-Class Exercise 2 - Visualising Distribution",
    "section": "Mapping Probabilities directly onto colour",
    "text": "Mapping Probabilities directly onto colour\nStat function called stat_density_ridges() replaces stat_density() of ggplot2.\nFigure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_df,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nInclude the argument calc_ecdf = TRUE in stat_density_ridges()!\n\n\n\nRidgeline Plots with Quantile Lines\nRidgeline plots can be coloured by quantile using geom_density_ridges_gradient(), via the calculated stat(quantile) \n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_df,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nQuantiles can also be specified by cut points e.g. 2.5% and 97.5% tails to colour the ridgeline plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_df,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "inclass/inclass02/inclass02.html#plotting-a-half-eye-graph",
    "href": "inclass/inclass02/inclass02.html#plotting-a-half-eye-graph",
    "title": "In-Class Exercise 2 - Visualising Distribution",
    "section": "Plotting a Half Eye graph",
    "text": "Plotting a Half Eye graph\nPlot a Half-Eye graph by using stat_halfeye() of ggdist package, producing a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemove the slab interval by setting .width = 0 and point_colour = NA."
  },
  {
    "objectID": "inclass/inclass02/inclass02.html#adding-the-boxplot",
    "href": "inclass/inclass02/inclass02.html#adding-the-boxplot",
    "title": "In-Class Exercise 2 - Visualising Distribution",
    "section": "Adding the boxplot",
    "text": "Adding the boxplot\nThe second geometry layer i.e. a narrow boxplot is produced using geom_boxplot() of ggplot2 This produces a narrow boxplot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_df, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)"
  },
  {
    "objectID": "inclass/inclass02/inclass02.html#adding-the-dot-plots",
    "href": "inclass/inclass02/inclass02.html#adding-the-dot-plots",
    "title": "In-Class Exercise 2 - Visualising Distribution",
    "section": "Adding the Dot Plots",
    "text": "Adding the Dot Plots\nThe third geometry layer is added using stat_dots() of ggdist package. This produces a half-dotplot, similar to a histogram that indicates the number of samples (number of dots) in each bin. Use side = “left” to specify the dot plots on the left-hand side.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_df, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)"
  },
  {
    "objectID": "inclass/inclass02/inclass02.html#finishing-touch",
    "href": "inclass/inclass02/inclass02.html#finishing-touch",
    "title": "In-Class Exercise 2 - Visualising Distribution",
    "section": "Finishing touch",
    "text": "Finishing touch\n coord_flip() of ggplot2 package is used to flip the raincloud chart horizontally to give it the raincloud appearance. theme_economist() of ggthemes package is also used to give the raincloud chart a professional publishing standard look.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_df, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_1.html",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_1.html",
    "title": "Hands-on Exercise 8 Part I - Choropleth Mapping with R",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_1.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_1.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 8 Part I - Choropleth Mapping with R",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\nTidyverse:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\n\nsf for handling geospatial data\ntmap for plotting choropleth maps\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_1.html#importing-data-into-r",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_1.html#importing-data-into-r",
    "title": "Hands-on Exercise 8 Part I - Choropleth Mapping with R",
    "section": "Importing Data into R",
    "text": "Importing Data into R\n\nThe Data\nTwo data sets will be used to create the choropleth map:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format downloaded from data.gov.sg This geospatial data consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data file downloaded from Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\nImporting Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\kekekay\\ISSS608-VAA\\Hands_on_exercise\\Hands_on_ex8\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\nExamining the data content\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\nImporting Attribute Data into R\nUse read_csv() function of readr package to import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\nData Preparation\nBefore a thematic map can be prepared, a data table with year 2020 values needs to be prepped. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\nData wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\nJoining the attribute data and geospatial data\nThe values of PA and SZ fields are a mix of upper and lower case. However, the values of SUBZONE_N and PLN_AREA_N are in upper case. Thus, we convert those of PA and SZ to uppercase, before we proceed with the join.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nleft_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\n\n\n\n\nNote\n\n\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nSave output into rds file\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\nChoropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\nPlotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\n\n\nCreating a choropleth map by using tmap’s elements\nTo draw a high quality and highly customisable cartographic choropleth map, tmap’s drawing elements should be used.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\nDrawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\nDrawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, the target variable such as Dependency needs to be assigned to tm_polygons().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe default interval binning used to draw the choropleth map is called “pretty”.\nThe default colour scheme used is YlOrRd of ColorBrewer.\nBy default, Missing value will be shaded in grey.\n\n\n\n\nDrawing a choropleth map using tm_fill() and tm_border()\ntm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone. Note that the planning subzones are shared according to the respective dependecy values.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_borders will be used to add the boundary of the planning subzones.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that light-gray border lines have been added to the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value is 1.\nBesides alpha argument, there are three other arguments for tm_borders():\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n\n\nData classification methods of tmap\nMost choropleth maps employ some methods of data classification to take a large number of observations and group them into data ranges or classes.\ntmap provides a ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() is used.\n\nPlotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification with 5 classes.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn the code chunk below, the equal data classification method is used.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTthe distribution of quantile data classification method are more evenly distributed then equal data classification method\n\n\n\n\nPlotting choropleth map with custome break\nFor all built-in styles, category breaks are computed internally. To override these defaults, the breakpoints can be set explicitly using the breaks argument to the tm_fill().\nNote: tmap breaks include a minimum and maximum. Thus, to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nCode chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field in order to get descriptive statistics on the variable to aid setting break points.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nThe choropleth map is plotted with the above values by the code chunk below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nColour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\nUsing ColourBrewer palette\nTo change the colour, assign the preferred colour to palette argument of tm_fill()\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nTo reverse the colour shading, add a “-” prefix.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nMap Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements includes: objects to be mapped, title, scale bar, compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\nMap Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nMap style\ntmap allows a wide variety of layout settings to be changed using tmap_style().\nThe code chunk below shows the classic style.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\nCartographic Furniture\nBeside map style, tmap also also draws other map furniture e.g., compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style\n\ntmap_style(\"white\")\n\n\n\n\nDrawing Small Multiple Choropleth Maps\nSmall multiple maps, aka facet maps, are composed of many maps arranged side-by-side, and/or stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, e.g. time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments\nby defining a group-by variable in tm_facets()\nby creating multiple stand-alone maps with tmap_arrange().\n\n\nAssigning multiple values to at least one of the aesthetic arguments\nSmall multiple choropleth maps are created by defining ncols in tm_fill()\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nsmall multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\nDefining a group-by variable in tm_facets()\nmultiple small choropleth maps are created by using tm_facets().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nCreating multiple stand-alone maps with tmap_arrange()\nmultiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\nMappping Spatial Object Meeting a Selection Criterion\nUse selection funtion to map spatial objects meeting the selection criterion.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html",
    "href": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html",
    "title": "Hands-on Exercise 6 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 6 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\nigraph\ntidygraph\nggraph\nvisNetwork\ntidyverse\nlubridate\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#the-data",
    "href": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#the-data",
    "title": "Hands-on Exercise 6 - Modelling, Visualising and Analysing Network Data with R",
    "section": "The Data",
    "text": "The Data\nThe data sets used are from an oil exploration and extraction company. There are two data sets:\n\nNodes data: GAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\nedges (aka link) data: GAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\nImporting network data from files\nImport GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\nReviewing the imported data\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\nWrangling time\nThe output report of GAStech_edges below reveals that the SentDate is treated as “Character” data type instead of date data type, which is incorrect. Thus, the data type of SentDate field needs to be changed to “Date”” data type.\nNote:\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\nReviewing the revised date fields\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\nWrangling attributes\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records, which is not very useful for visualisation.\nThus, aggregation is done for the individual by date, senders, receivers, main subject and day of the week.\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\nReviewing the revised edges file\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#creating-network-objects-using-tidygraph",
    "href": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Exercise 6 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Creating network objects using tidygraph",
    "text": "Creating network objects using tidygraph\n tidygraph package provides a tidy API for graph/network manipulation. Network data can be envisioned as 2 tidy tables, 1 for node data and 1 for edge data.\ntidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. It is also provides access to many graph algorithms with return values that facilitate their use in a tidy workflow.\n\nThe tbl_graph object\ntidygraph can be used to create network objects:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\nThe dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n.N() function is used to gain access to the node data while manipulating the edge data. E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\nUsing tbl_graph() to build tidygraph data model.\nuse tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\nReviewing the output tidygraph’s graph object\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\nReviewing the output tidygraph’s graph object\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\nChanging the active object\nThe nodes tibble data frame is activated by default, but it can be changed with the activate() function. To rearrange the rows in the edges tibble to list those with the highest “weight” first, use activate() and then arrange().\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on Exercise 6 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Plotting Static Network Graphs with ggraph package",
    "text": "Plotting Static Network Graphs with ggraph package\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nThere are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts\n\n\nPlotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. \n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired.\nBoth of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\nChanging the default network graph theme\nUse theme_graph() to remove the x and y axes.\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nChanging the coloring of the plot\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\n\n\nWorking with ggraph’s layouts\nggraph supports many standard layouts: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\nFruchterman and Reingold layout\nPlot the network graph using Fruchterman and Reingold layout.\n\nlayout argument is used to define the layout to be used.\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nModifying network nodes\ncolour each node by referring to their respective departments\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes.\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nModifying edges\nThe thickness of the edges will be mapped with the Weight variable.\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#creating-facet-graphs",
    "href": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#creating-facet-graphs",
    "title": "Hands-on Exercise 6 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Creating facet graphs",
    "text": "Creating facet graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\nWorking with facet_edges()\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\nWorking with facet_edges()\nuses theme() to change the position of the legend\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\nA framed facet graph\nAdd frame to each graph\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\nWorking with facet_nodes()\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#network-metrics-analysis",
    "href": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#network-metrics-analysis",
    "title": "Hands-on Exercise 6 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Network Metrics Analysis",
    "text": "Network Metrics Analysis\n\nComputing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here.\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nVisualising network metrics\nFrom ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nVisualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal).\ngroup_edge_betweenness() is used below\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands_on_exercise/Hands_on_ex6/Hands-on_Ex06.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on Exercise 6 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Building Interactive Network Graph with visNetwork",
    "text": "Building Interactive Network Graph with visNetwork\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nResulting graph:\n\nThe nodes can be moved and the graph will use an algorithm to keep the nodes properly spaced.\nThe plot can be moved around to be re-centered, zoomed in and out.\n\n\n\nData preparation\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\nPlotting the first interactive network graph\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\nWorking with layout\nFruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\n\nWorking with visual attributes - Nodes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nrename Department field to group\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nvisNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\nWorking with visual attributes - Edges\nvisEdges() is used to symbolise the edges.\n\nThe argument arrows is used to define where to place the arrow.\nThe smooth argument is used to plot the edges using a smooth curve.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\nInteractivity\nvisOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html",
    "title": "Hands-on Exercise 4 - Funnel Plots for Fair Comparisons",
    "section": "",
    "text": "Funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities.\nLearning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 4 - Funnel Plots for Fair Comparisons",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\npackage 'FunnelPlotR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\idrin\\AppData\\Local\\Temp\\RtmpeAwTUc\\downloaded_packages"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#importing-the-data",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#importing-the-data",
    "title": "Hands-on Exercise 4 - Funnel Plots for Fair Comparisons",
    "section": "Importing the Data",
    "text": "Importing the Data\nThe COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal.\nFor this hands-on exercise, compares the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#funnelplotr-methods-the-basic-plot",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#funnelplotr-methods-the-basic-plot",
    "title": "Hands-on Exercise 4 - Funnel Plots for Fair Comparisons",
    "section": "FunnelPlotR methods: The basic plot",
    "text": "FunnelPlotR methods: The basic plot\nThe code chunk below plots a funnel plot.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\nfunnel_plot(\n  .data = covid19,\n  numerator = `Positive`,\n  denominator = `Death`,\n  group = `Sub-district`\n)\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. Plot is adjusted for overdispersion."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#funnelplotr-methods-makeover-1",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#funnelplotr-methods-makeover-1",
    "title": "Hands-on Exercise 4 - Funnel Plots for Fair Comparisons",
    "section": "FunnelPlotR methods: Makeover 1",
    "text": "FunnelPlotR methods: Makeover 1\nThe code chunk below plots a funnel plot.\n\ndata_type argument is used to change from default “SR” to “PR” (i.e. proportions).\nx_range and y_range are used to set the range of x-axis and y-axis\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\nfunnel_plot(\n  .data = covid19,\n  numerator = `Death`,\n  denominator = `Positive`,\n  group = `Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  x_range = c(0, 6500),  #&lt;&lt;\n  y_range = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. Plot is adjusted for overdispersion."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#funnelplotr-methods-makeover-2",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#funnelplotr-methods-makeover-2",
    "title": "Hands-on Exercise 4 - Funnel Plots for Fair Comparisons",
    "section": "FunnelPlotR methods: Makeover 2",
    "text": "FunnelPlotR methods: Makeover 2\nThe code chunk below plots a funnel plot.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\nfunnel_plot(\n  .data = covid19,\n  numerator = `Death`,\n  denominator = `Positive`,\n  group = `Sub-district`,\n  data_type = \"PR\",\n  x_range = c(0, 6500),  \n  y_range = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)    \n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. Plot is adjusted for overdispersion."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#computing-the-basic-derived-fields",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#computing-the-basic-derived-fields",
    "title": "Hands-on Exercise 4 - Funnel Plots for Fair Comparisons",
    "section": "Computing the basic derived fields",
    "text": "Computing the basic derived fields\nFirst, derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#calculate-lower-and-upper-limits-for-95-and-99.9-ci",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#calculate-lower-and-upper-limits-for-95-and-99.9-ci",
    "title": "Hands-on Exercise 4 - Funnel Plots for Fair Comparisons",
    "section": "Calculate lower and upper limits for 95% and 99.9% CI",
    "text": "Calculate lower and upper limits for 95% and 99.9% CI\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#plotting-a-static-funnel-plot",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#plotting-a-static-funnel-plot",
    "title": "Hands-on Exercise 4 - Funnel Plots for Fair Comparisons",
    "section": "Plotting a static funnel plot",
    "text": "Plotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#interactive-funnel-plot-plotly-ggplot2",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_3.html#interactive-funnel-plot-plotly-ggplot2",
    "title": "Hands-on Exercise 4 - Funnel Plots for Fair Comparisons",
    "section": "Interactive Funnel Plot: plotly + ggplot2",
    "text": "Interactive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\nggstatsplotis an extension of ggplot2 package for creating graphics with details from statstical tests included in the plots themselces\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#importing-the-data",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#importing-the-data",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Importing the Data",
    "text": "Importing the Data\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package.\nreadr is a pacakge within tidyverse.\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\nexam_data tibble data frame contains:\n\nYear end examination grades of a cohort of primary 3 students from a local school.\nThere are a total of seven attributes. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#one-sample-test-gghistostats-method",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#one-sample-test-gghistostats-method",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "One-sample test: gghistostats() method",
    "text": "One-sample test: gghistostats() method\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(1234) # for reproducibility\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#unpacking-the-bayes-factor",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#unpacking-the-bayes-factor",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Unpacking the Bayes Factor",
    "text": "Unpacking the Bayes Factor\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. i.e., a measure of the strength of evidence in favor of one theory among two competing theories.\nBayes factor allows evaluation of the data in favor of a null hypothesis, and to use external information to do so. It gives the weight of the evidence in favor of a given hypothesis.\nWhen comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10, defined mathematically as:\n\\(\\frac{likelihood of data given H_{1}}{likelihood of data given H_{0}} = \\frac{P(D|H_{1})}{P(D|H_{0})}\\)\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\nInterpreting Bayes Factor\nA Bayes Factor can be any positive number. A common interpretation was first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\nB10 Value\nConclusion\n\n\n\n\n&gt;100\nExtreme evidence for H1\n\n\n30-100\nVery strong evidence for H1\n\n\n10-30\nStrong evidence for H1\n\n\n3-10\nModerate evidence for H1\n\n\n1-3\nAnecdotal evidence for H1\n\n\n1\nNo evidence\n\n\n1/3-1\nAnecdotal evidence for H1\n\n\n1/3-1/10\nModerate evidence for H1\n\n\n1/10-1/30\nStrong evidence for H1\n\n\n1/30-1/100\nVery strong evidence for H1\n\n\n&lt;1/100\nExtreme evidence for H1"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#two-sample-mean-test-ggbetweenstats",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#two-sample-mean-test-ggbetweenstats",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Two-sample mean test: ggbetweenstats()",
    "text": "Two-sample mean test: ggbetweenstats()\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#oneway-anova-test-ggbetweenstats-method",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#oneway-anova-test-ggbetweenstats-method",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Oneway ANOVA Test: ggbetweenstats() method",
    "text": "Oneway ANOVA Test: ggbetweenstats() method\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\nggbetweenstats - Summary of tests\nFollowing (between-subjects) tests are carried out for each type of analyses:\n\n\n\n\n\n\n\n\nType\nNo. of groups\nTest\n\n\n\n\nParametric\n&gt;2\nFisher’s or Welch’s one-way ANOVA\n\n\nNon-parametric\n&gt;2\nKruskal-Wallis one-way ANOVA\n\n\nRobust\n&gt;2\nHeteroscedastic one-way ANOVA for trimmed means\n\n\nBayes Factor\n&gt;2\nFisher’s ANOVA\n\n\nParametric\n2\nStudent’s or Welch’s t-test\n\n\nNon-parametric\n2\nMann-Whitney U test\n\n\nRobust\n2\nYuen’s test for trimmed means\n\n\nBayes Factor\n2\nStudent’s t-test\n\n\n\nFollowing effect sizes (and confidence intervals) are available for each type of test:\n\n\n\n\n\n\n\n\n\nType\nNo. of Groups\nEffect Size\nConfidence Intervals\n\n\n\n\nParametric\n&gt;2\n\\({\\eta^2}_{p},\\eta^2,{\\omega^2}_{p},\\omega^2\\)\nYes\n\n\nNon-parametric\n&gt;2\n\\({\\eta^2}_{H}\\) (H-statistic based eta-squared)\nYes\n\n\nRobust\n&gt;2\n\\(\\xi\\) (Explanatory measure of effect size)\nYes\n\n\nBayes Factor\n&gt;2\nNo\nNo\n\n\nParametric\n2\nCohen’s d, Hedge’s g (central-and-noncentral-t distribution based)\nYes\n\n\nNon-parametric\n2\nr (computed as \\(Z/\\sqrt{N}\\))\nYes\n\n\nRobust\n2\n\\(\\xi\\) (Explanatory measure of effect size)\nYes\n\n\nBayes Factor\n2\nNo\nNo\n\n\n\nSummary of pairwise comparison tests supported in ggbetweenstats\n\n\n\n\n\n\n\n\n\nType\nEqual Variance\nTest\np-value Adjustment?\n\n\n\n\nParametric\nNo\nGames Howell Test\nYes\n\n\nParametric\nYes\nStudent’s t Test\nYes\n\n\nNon-Parametric\nNo\nDunn Test\nYes\n\n\nRobust\nNo\nYuen’s Trimmed Means Test\nYes\n\n\nBayes Factor\nNA\nStudent’s t Test\nNA"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#significant-test-of-correlation-ggscatterstats",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#significant-test-of-correlation-ggscatterstats",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Significant Test of Correlation: ggscatterstats()",
    "text": "Significant Test of Correlation: ggscatterstats()\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#significant-test-of-association-depedence-ggbarstats-methods",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#significant-test-of-association-depedence-ggbarstats-methods",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Significant Test of Association (Depedence) : ggbarstats() methods",
    "text": "Significant Test of Association (Depedence) : ggbarstats() methods\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut(). ggbarstats() is used to build a visual for Significant Test of Association\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#visualising-models",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#visualising-models",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Visualising Models",
    "text": "Visualising Models\nVisualise model diagnostic and model parameters by using parameters package.\n\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#installing-and-loading-the-required-libraries-1",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#installing-and-loading-the-required-libraries-1",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\n\npacman::p_load(readxl, performance, parameters, see)\n\n\nImporting Excel file: readxl methods\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#multiple-regression-model-using-lm",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#multiple-regression-model-using-lm",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Multiple Regression Model using lm()",
    "text": "Multiple Regression Model using lm()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#model-diagnostic-checking-for-multicolinearity",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#model-diagnostic-checking-for-multicolinearity",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Model Diagnostic: checking for multicolinearity:",
    "text": "Model Diagnostic: checking for multicolinearity:\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#model-diagnostic-checking-normality-assumption",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#model-diagnostic-checking-normality-assumption",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Model Diagnostic: checking normality assumption",
    "text": "Model Diagnostic: checking normality assumption\nIn the code chunk, check_normality() of performance package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\ncheck_n &lt;- check_normality(model1)\n\nplot(check_n)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#model-diagnostic-check-model-for-homogeneity-of-variances",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#model-diagnostic-check-model-for-homogeneity-of-variances",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Model Diagnostic: Check model for homogeneity of variances",
    "text": "Model Diagnostic: Check model for homogeneity of variances\nIn the code chunk, check_heteroscedasticity() of performance package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncheck_h &lt;- check_heteroscedasticity(model1)\nplot(check_h)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#model-diagnostic-complete-check",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#model-diagnostic-complete-check",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Model Diagnostic: Complete check",
    "text": "Model Diagnostic: Complete check\nWe can also perform the complete by using check_model().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncheck_model(model1)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#visualising-regression-parameters-see-methods",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#visualising-regression-parameters-see-methods",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Visualising Regression Parameters: see methods",
    "text": "Visualising Regression Parameters: see methods\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(parameters(model1))"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#visualising-regression-parameters-ggcoefstats-methods",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_1.html#visualising-regression-parameters-ggcoefstats-methods",
    "title": "Hands-on Exercise 4 - Visual Statistical Analysis",
    "section": "Visualising Regression Parameters: ggcoefstats() methods",
    "text": "Visualising Regression Parameters: ggcoefstats() methods\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\nggiraph for making ‘ggplot’ graphics interactive.\nplotly, R library for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork for combining multiple ggplot2 graphs into one figure.\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#importing-the-data",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#importing-the-data",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Importing the Data",
    "text": "Importing the Data\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package.\nreadr is a pacakge within tidyverse.\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nexam_data tibble data frame contains:\n\nYear end examination grades of a cohort of primary 3 students from a local school.\nThere are a total of seven attributes. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#tooltip-effect-with-tooltip-aesthetic",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#tooltip-effect-with-tooltip-aesthetic",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Tooltip effect with tooltip aesthetic",
    "text": "Tooltip effect with tooltip aesthetic\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. \nFirst, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#displaying-multiple-information-on-tooltip",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#displaying-multiple-information-on-tooltip",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Displaying multiple information on tooltip",
    "text": "Displaying multiple information on tooltip\nThe first three lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7. By hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#customising-tooltop-style",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#customising-tooltop-style",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Customising Tooltop style",
    "text": "Customising Tooltop style\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\nNotice that the background colour of the tooltip is black and the font colour is white and bold.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)                                        \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor more customisations, refer to Customizing girafe objects"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#displaying-statistics-on-tooltip",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#displaying-statistics-on-tooltip",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Displaying statistics on tooltip",
    "text": "Displaying statistics on tooltip\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#hover-effect-with-data_id-aesthetic",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#hover-effect-with-data_id-aesthetic",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Hover effect with data_id aesthetic",
    "text": "Hover effect with data_id aesthetic\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\nElements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe default value of the hover css is hover_css = “fill:orange;”."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#styling-hover-effect",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#styling-hover-effect",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Styling hover effect",
    "text": "Styling hover effect\nIn the code chunk below, css codes are used to change the highlighting effect.\nElements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#plot-1-combining-tooltip-and-hover-effect",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#plot-1-combining-tooltip-and-hover-effect",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Plot 1: Combining tooltip and hover effect",
    "text": "Plot 1: Combining tooltip and hover effect\nThe tooltip and hover effects are combined in the interactive statistical graph in the code chunk below.\nElements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#click-effect-with-onclick",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#click-effect-with-onclick",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Click effect with onclick",
    "text": "Click effect with onclick\nonclick argument of ggiraph provides hotlink interactivity on the web. Web document link with a data object will be displayed on the web browser upon mouse click.\nThe code chunk below shown an example of onclick.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                        \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nClick actions must be a string column in the dataset containing valid javascript instructions."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#coordinated-multiple-views-with-ggiraph",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#coordinated-multiple-views-with-ggiraph",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Coordinated Multiple Views with ggiraph",
    "text": "Coordinated Multiple Views with ggiraph\nWhen a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\npatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       )"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#there-are-two-ways-to-create-interactive-graph-by-using-plotly-they-are",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#there-are-two-ways-to-create-interactive-graph-by-using-plotly-they-are",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "There are two ways to create interactive graph by using plotly, they are:",
    "text": "There are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#creating-an-interactive-scatter-plot-plot_ly-method",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#creating-an-interactive-scatter-plot-plot_ly-method",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Creating an interactive scatter plot: plot_ly() method",
    "text": "Creating an interactive scatter plot: plot_ly() method\nThe code chunk below shows an example a basic interactive plot created by using plot_ly().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#working-with-visual-variable-plot_ly-method",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#working-with-visual-variable-plot_ly-method",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Working with visual variable: plot_ly() method",
    "text": "Working with visual variable: plot_ly() method\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#creating-an-interactive-scatter-plot-ggplotly-method",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#creating-an-interactive-scatter-plot-ggplotly-method",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Creating an interactive scatter plot: ggplotly() method",
    "text": "Creating an interactive scatter plot: ggplotly() method\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe only extra line you need to include in the code chunk is ggplotly()."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#interactive-data-visualisation---crosstalk-methods",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#interactive-data-visualisation---crosstalk-methods",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Interactive Data Visualisation - crosstalk methods",
    "text": "Interactive Data Visualisation - crosstalk methods\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\nDT::datatable(exam_data, class= \"compact\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#linked-brushing-crosstalk-method",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_1.html#linked-brushing-crosstalk-method",
    "title": "Hands-on Exercise 3 - I Programming Interactive Data Visualisation with R",
    "section": "Linked brushing: crosstalk method",
    "text": "Linked brushing: crosstalk method\nCode chunk below is used to implement coordinated brushing.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html",
    "href": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html",
    "title": "Hands-on ex1",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if tidyverse pakcges are installed in the computer. If yes, then they will be launched into R environment.\n\n\nCode\npacman::p_load(tidyverse)\n\n\n\n\n\n\n\nCode\ngetwd()\n\n\n[1] \"C:/kekekay/ISSS608-VAA/Hands_on_exercise/Hands_on_ex1\"\n\n\n\n\nCode\nexam_data &lt;- read_csv(\"Exam_data.csv\")\nexam_data\n\n\n# A tibble: 322 × 7\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Student321 3I    Male   Malay        21     9      15\n 2 Student305 3I    Female Malay        24    22      16\n 3 Student289 3H    Male   Chinese      26    16      16\n 4 Student227 3F    Male   Chinese      27    77      31\n 5 Student318 3I    Male   Malay        27    11      25\n 6 Student306 3I    Female Malay        31    16      16\n 7 Student313 3I    Male   Chinese      31    21      25\n 8 Student316 3I    Male   Malay        31    18      27\n 9 Student312 3I    Male   Malay        33    19      15\n10 Student297 3H    Male   Indian       34    49      37\n# ℹ 312 more rows\n\n\n\n\n\n\n\nCode\nggplot(data = exam_data,\n       aes(x=RACE)) +\n  geom_bar()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#install-and-launching-r-packages",
    "href": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#install-and-launching-r-packages",
    "title": "Hands-on ex1",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if tidyverse pakcges are installed in the computer. If yes, then they will be launched into R environment.\n\n\nCode\npacman::p_load(tidyverse)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#checking-working-directory-and-importing-data",
    "href": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#checking-working-directory-and-importing-data",
    "title": "Hands-on ex1",
    "section": "",
    "text": "Code\ngetwd()\n\n\n[1] \"C:/kekekay/ISSS608-VAA/Hands_on_exercise/Hands_on_ex1\"\n\n\n\n\nCode\nexam_data &lt;- read_csv(\"Exam_data.csv\")\nexam_data\n\n\n# A tibble: 322 × 7\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Student321 3I    Male   Malay        21     9      15\n 2 Student305 3I    Female Malay        24    22      16\n 3 Student289 3H    Male   Chinese      26    16      16\n 4 Student227 3F    Male   Chinese      27    77      31\n 5 Student318 3I    Male   Malay        27    11      25\n 6 Student306 3I    Female Malay        31    16      16\n 7 Student313 3I    Male   Chinese      31    21      25\n 8 Student316 3I    Male   Malay        31    18      27\n 9 Student312 3I    Male   Malay        33    19      15\n10 Student297 3H    Male   Indian       34    49      37\n# ℹ 312 more rows"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#plotting-a-simple-bar-chart",
    "href": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#plotting-a-simple-bar-chart",
    "title": "Hands-on ex1",
    "section": "",
    "text": "Code\nggplot(data = exam_data,\n       aes(x=RACE)) +\n  geom_bar()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#comparison-of-base-r-and-ggplot-plot",
    "href": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#comparison-of-base-r-and-ggplot-plot",
    "title": "Hands-on ex1",
    "section": "2.1 Comparison of Base R and ggplot plot",
    "text": "2.1 Comparison of Base R and ggplot plot\n\nCode\nlibrary(ggplot2)\n\n#R graphic \nhist(exam_data$MATHS)\n\n#ggplot\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"grey\") +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes:\n\n\n\n\n\nWhile both approaches achieve similar results, the ggplot2 version can be easily modified and extended with additional layers, themes, and scales without fundamentally altering the underlying code structure. more suitable for complex visualization."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#call-the-ggplot-function",
    "href": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#call-the-ggplot-function",
    "title": "Hands-on ex1",
    "section": "3.1 call the ggplot() function",
    "text": "3.1 call the ggplot() function\n\n\nCode\nggplot(data=exam_data)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#adding-in-x-axis-and-the-axiss-label",
    "href": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#adding-in-x-axis-and-the-axiss-label",
    "title": "Hands-on ex1",
    "section": "3.2 adding in x-axis and the axis’s label",
    "text": "3.2 adding in x-axis and the axis’s label\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS))"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#essential-grammatical-elements-in-ggplot2-geom",
    "href": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#essential-grammatical-elements-in-ggplot2-geom",
    "title": "Hands-on ex1",
    "section": "3.3 Essential Grammatical Elements in ggplot2: geom",
    "text": "3.3 Essential Grammatical Elements in ggplot2: geom\n\n\nCode\n#| fig-cap: \"Geometric Objects: geom_bar VS geom_dotplot vs geom_histogram()\" \n\n#| fig-subcap: \n#| - \"plots a bar chart by using geom_bar()\" \n\n#| - \"geom_dotplot() of ggplot2 is used to plot a dot plot\" \n#| - \"turn off the y-axis &  change the binwidth to 2.5\"\n#| - \"create a simple histogram\"\n#| layout-ncol: 4 \n#| column: page\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(dotsize = 0.5)\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,         \n               dotsize = 0.5) +      \n  scale_y_continuous(NULL,           \n                     breaks = NULL)  \n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram()       \n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\nCode\n#| fig-cap: \"Modifying a Geometric Object \" \n\n#| fig-subcap: \n#| - \"by changing geom()\" \n\n#| - \"by changing aes()\" \n\n#| layout-ncol: 2\n#| column: page\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\")  \n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"grey30\")\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\n\n\nCode\n#| fig-cap: \"Geometric Objects: geom-density() VS geom_boxplot()\" \n\n#| fig-subcap: \n#| - \"geom-density() computes and plots kernel density estimate\" \n\n#| - \"two kernel density lines by using colour or fill arguments of aes()\" \n#| - \"geom_boxplot() displays continuous value list\"\n#| - \"adding notch to help visually assess whether the medians of distributions differ\"\n#| layout-ncol: 4 \n#| column: page\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_density()           \n\nggplot(data=exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density()\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()            \n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)    \n\n\n\n\n\n\n\n\nFigure 7\n\n\n\n\n\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\n\n\n\n\n\n\nFigure 9\n\n\n\n\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\n\n\nCode\n#| fig-cap: \"Geometric Objects: geom_violin VS geom_point()\" \n\n#| fig-subcap: \n#| - \"geom_violin is designed for creating violin plot to compare several distributions side by side\" \n\n#| - \"geom_point() is especially useful for creating scatterplot\" \n\n#| layout-ncol: 2\n#| column: page\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point()            \n\n\n\n\n\n\n\n\nFigure 11\n\n\n\n\n\n\n\n\n\n\n\nFigure 12"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#combine-geom-objects",
    "href": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#combine-geom-objects",
    "title": "Hands-on ex1",
    "section": "3.4 Combine geom objects",
    "text": "3.4 Combine geom objects\nplot the data points on the boxplots by using both geom_boxplot() and geom_point().\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#essential-grammatical-elements-in-ggplot2-stat",
    "href": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#essential-grammatical-elements-in-ggplot2-stat",
    "title": "Hands-on ex1",
    "section": "3.5 Essential Grammatical Elements in ggplot2: stat",
    "text": "3.5 Essential Grammatical Elements in ggplot2: stat\n\n3.5.1 Working with stat()\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n3.5.2 using stat_summary() function and overriding the default geom\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun=\"mean\",         \n               colour =\"red\",        \n               size=4)               \n\n\n\n\n\n\n\n\n\n\n\n3.5.3 using geom_() function and overriding the default stat\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun=\"mean\",           \n             colour =\"red\",          \n             size=4)  \n\n\n\n\n\n\n\n\n\n\n\n3.5.4 adding a best fit curve on a scatterplot\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(linewidth=0.5)\n\n\n\n\n\n\n\n\n\n\n\n3.5.5 override default smoothing method from loess to linear model\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#essential-grammatical-elements-in-ggplot2-facets",
    "href": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#essential-grammatical-elements-in-ggplot2-facets",
    "title": "Hands-on ex1",
    "section": "3.6 Essential Grammatical Elements in ggplot2: Facets",
    "text": "3.6 Essential Grammatical Elements in ggplot2: Facets\n\n3.6.1 Working with facet_wrap()\nThe facet_grid() function create a grid of plots by specifying rows and columns based on factors.\n\n\nCode\n# Assuming exam_data is already loaded and contains columns MATHS and CLASS\nggplot(data=exam_data, aes(x=MATHS)) +\n  geom_histogram(bins=20) +\n  facet_grid(CLASS ~ .)  # CLASS variable defines the rows\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, aes(x=MATHS)) +\n  geom_histogram(bins=20) +\n  facet_grid(. ~ CLASS)  # CLASS variable defines the columns\n\n\n\n\n\n\n\n\n\nfacet_wrap wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid because most displays are roughly rectangular.\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#essential-grammatical-elements-in-ggplot2-coordinates",
    "href": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#essential-grammatical-elements-in-ggplot2-coordinates",
    "title": "Hands-on ex1",
    "section": "3.7 Essential Grammatical Elements in ggplot2: Coordinates",
    "text": "3.7 Essential Grammatical Elements in ggplot2: Coordinates\n\n3.7.1 Working with Coordinate\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n3.7.2 flips the horizontal bar chart into vertical bar chart by using coord_flip()\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n3.7.3 Changing the y- and x-axis range\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, size=0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe scatterplot on the right is slightly misleading because the y-aixs and x-axis range are not equal.\n\n\n\n\n3.7.4 fixed both the y-axis and x-axis range from 0-100.\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#essential-grammatical-elements-in-ggplot2-themes",
    "href": "Hands_on_exercise/Hands_on_ex1/Hands_on_ex1.html#essential-grammatical-elements-in-ggplot2-themes",
    "title": "Hands-on ex1",
    "section": "3.8 Essential Grammatical Elements in ggplot2: themes",
    "text": "3.8 Essential Grammatical Elements in ggplot2: themes\n\n3.8.1 Working with theme\n\n\nPlotData\n\n\n\n\nCode\nggplot(data=exam_data, aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot with theme_gray()\nggplot(data=exam_data, aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\n\n\n\n\nCode\n# Plot with theme_classic()\nggplot(data=exam_data, aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nCode\n# Plot with theme_minimal()\nggplot(data=exam_data, aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my ISSS608 Visual Analytic and Applications.\nReference :\nQuarto – HTML Theming\nReports & Presentations with Quarto\nQuarto Basics\nR for Visual Analytics"
  },
  {
    "objectID": "before_class.html",
    "href": "before_class.html",
    "title": "ISSS608-VA",
    "section": "",
    "text": "Welcome to my ISSS608 Visual Analytic and Applications.\nReference :\nQuarto – HTML Theming\nReports & Presentations with Quarto\nQuarto Basics\nR for Visual Analytics"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex2/Hands_on_ex2.html",
    "href": "Hands_on_exercise/Hands_on_ex2/Hands_on_ex2.html",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "pacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse) \n\n\n\n\n\ngetwd()\n\n[1] \"C:/kekekay/ISSS608-VAA/Hands_on_exercise/Hands_on_ex2\"\n\nexam_data &lt;- read_csv(\"Exam_data.csv\")\n\n\n\n\n\nPlotCode\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotCode\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\nLets try The Wall Street Journal.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_wsj()\n\n\n\n\n\n\n\n\n\n\n\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 8,\n              base_size = 20,\n              grid = \"Y\")\n\n\n\n\n\n\n\n\n\n\n\n\nhow to create composite plot by combining multiple graphs. First, let us create three statistical graphics by using the code chunk below.\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\nprint(p1)\n\n\n\n\n\n\n\nprint(p2)\n\n\n\n\n\n\n\nprint(p3)\n\n\n\n\n\n\n\n\n\n\nPatchwork package has a very simple syntax where we can create layouts super easily. Here’s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\n\n\n\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\n\n“/” operator to stack two ggplot2 graphs,\n“|” operator to place the plots beside each other,\n“()” operator the define the sequence of the plotting.\n\n\n(p1 / p2) | p3\n\n\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\n\n\n\n\nWith inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\np\n\nA grob, ggplot, patchwork, formula, raster, or nativeRaster object to add as an inset\n\nleft, bottom, right, top\n\nnumerics or units giving the location of the outer bounds. If given as numerics they will be converted to npc units.\n\nalign_to\n\nSpecifies what left, bottom, etc should be relative to. Either 'panel' (default), 'plot', or 'full'.\n\non_top\n\nLogical. Should the inset be placed on top of the other plot or below (but above the background)?\n\nclip\n\nLogical. Should clipping be performed on the inset?\n\nignore_tag\n\nLogical. Should autotagging ignore the inset?\n\n\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n\n\n\n\n\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_wsj()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex2/Hands_on_ex2.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex2/Hands_on_ex2.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "pacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex2/Hands_on_ex2.html#importing-data",
    "href": "Hands_on_exercise/Hands_on_ex2/Hands_on_ex2.html#importing-data",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "getwd()\n\n[1] \"C:/kekekay/ISSS608-VAA/Hands_on_exercise/Hands_on_ex2\"\n\nexam_data &lt;- read_csv(\"Exam_data.csv\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex2/Hands_on_ex2.html#beyond-ggplot2-annotation-ggrepel",
    "href": "Hands_on_exercise/Hands_on_ex2/Hands_on_ex2.html#beyond-ggplot2-annotation-ggrepel",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "PlotCode\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex2/Hands_on_ex2.html#beyond-ggplot2-themes",
    "href": "Hands_on_exercise/Hands_on_ex2/Hands_on_ex2.html#beyond-ggplot2-themes",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "PlotCode\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\nLets try The Wall Street Journal.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_wsj()\n\n\n\n\n\n\n\n\n\n\n\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 8,\n              base_size = 20,\n              grid = \"Y\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex2/Hands_on_ex2.html#beyond-single-graph",
    "href": "Hands_on_exercise/Hands_on_ex2/Hands_on_ex2.html#beyond-single-graph",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "how to create composite plot by combining multiple graphs. First, let us create three statistical graphics by using the code chunk below.\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\nprint(p1)\n\n\n\n\n\n\n\nprint(p2)\n\n\n\n\n\n\n\nprint(p3)\n\n\n\n\n\n\n\n\n\n\nPatchwork package has a very simple syntax where we can create layouts super easily. Here’s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\n\n\n\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\n\n“/” operator to stack two ggplot2 graphs,\n“|” operator to place the plots beside each other,\n“()” operator the define the sequence of the plotting.\n\n\n(p1 / p2) | p3\n\n\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\n\n\n\n\nWith inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\np\n\nA grob, ggplot, patchwork, formula, raster, or nativeRaster object to add as an inset\n\nleft, bottom, right, top\n\nnumerics or units giving the location of the outer bounds. If given as numerics they will be converted to npc units.\n\nalign_to\n\nSpecifies what left, bottom, etc should be relative to. Either 'panel' (default), 'plot', or 'full'.\n\non_top\n\nLogical. Should the inset be placed on top of the other plot or below (but above the background)?\n\nclip\n\nLogical. Should clipping be performed on the inset?\n\nignore_tag\n\nLogical. Should autotagging ignore the inset?\n\n\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n\n\n\n\n\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_wsj()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html",
    "title": "Hands-on Exercise 3 - Programming Animated Statistical Graphics with R",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#terminology",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#terminology",
    "title": "Hands-on Exercise 3 - Programming Animated Statistical Graphics with R",
    "section": "Terminology",
    "text": "Terminology\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The settings that control how the animation behaves. For example:\n\nDuration of each frame\nEasing function used between frame transitions\nStart the animation from the current frame or from the beginning"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 3 - Programming Animated Statistical Graphics with R",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#importing-the-data",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#importing-the-data",
    "title": "Hands-on Exercise 3 - Programming Animated Statistical Graphics with R",
    "section": "Importing the Data",
    "text": "Importing the Data\n\nThe code chunk below imports GlobalPopulation.xlsx into R environment by using read_xls() function of readr package.\nreadr is a pacakge within tidyverse.\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\n\n\n\nTip\n\n\n\nInstead of using mutate_at(), across() can be used to derive the same output"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#building-a-static-population-bubble-plot",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#building-a-static-population-bubble-plot",
    "title": "Hands-on Exercise 3 - Programming Animated Statistical Graphics with R",
    "section": "Building a static population bubble plot",
    "text": "Building a static population bubble plot\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young')"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#building-the-animated-bubble-plot",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#building-the-animated-bubble-plot",
    "title": "Hands-on Exercise 3 - Programming Animated Statistical Graphics with R",
    "section": "Building the animated bubble plot",
    "text": "Building the animated bubble plot\nIn the code chunk below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics.\n\nThe default is linear.\nOther methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#building-an-animated-bubble-plot-ggplotly-method",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#building-an-animated-bubble-plot-ggplotly-method",
    "title": "Hands-on Exercise 3 - Programming Animated Statistical Graphics with R",
    "section": "Building an animated bubble plot: ggplotly() method",
    "text": "Building an animated bubble plot: ggplotly() method\nCreate an animated bubble plot by using ggplotly() method.\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg.\nggplotly() is then used to convert the R graphic object into an animated svg object.\n\nAlthough show.legend = FALSE argument was used, the legend still appears on the plot. To overcome this problem, theme(legend.position='none') should be used\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#building-an-animated-bubble-plot-plot_ly-method",
    "href": "Hands_on_exercise/Hands_on_ex3/Hands-on_Ex03_2.html#building-an-animated-bubble-plot-plot_ly-method",
    "title": "Hands-on Exercise 3 - Programming Animated Statistical Graphics with R",
    "section": "Building an animated bubble plot: plot_ly() method",
    "text": "Building an animated bubble plot: plot_ly() method\nCreate an animated bubble plot by using plot_ly() method.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html",
    "title": "Hands-on Exercise 4 - Visualising Uncertainty",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 4 - Visualising Uncertainty",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\nstrapgod (NA    -&gt; ea2b1ecfc...) [GitHub]\nrlang    (1.1.3 -&gt; 1.1.4       ) [CRAN]\ncli      (3.6.2 -&gt; 3.6.3       ) [CRAN]\nstringi  (1.8.3 -&gt; 1.8.4       ) [CRAN]\nfarver   (2.1.1 -&gt; 2.1.2       ) [CRAN]\nmvtnorm  (1.2-4 -&gt; 1.2-5       ) [CRAN]\nrlang   (1.1.3 -&gt; 1.1.4) [CRAN]\ncli     (3.6.2 -&gt; 3.6.3) [CRAN]\nstringi (1.8.3 -&gt; 1.8.4) [CRAN]\npackage 'rlang' successfully unpacked and MD5 sums checked\npackage 'cli' successfully unpacked and MD5 sums checked\npackage 'stringi' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\idrin\\AppData\\Local\\Temp\\Rtmp2HmPAQ\\downloaded_packages\n── R CMD build ─────────────────────────────────────────────────────────────────\n* checking for file 'C:\\Users\\idrin\\AppData\\Local\\Temp\\Rtmp2HmPAQ\\remotes6c3c22a36614\\DavisVaughan-strapgod-ea2b1ec/DESCRIPTION' ... OK\n* preparing 'strapgod':\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\nOmitted 'LazyData' from DESCRIPTION\n* building 'strapgod_0.0.4.9000.tar.gz'\n\npackage 'rlang' successfully unpacked and MD5 sums checked\npackage 'cli' successfully unpacked and MD5 sums checked\npackage 'stringi' successfully unpacked and MD5 sums checked\npackage 'farver' successfully unpacked and MD5 sums checked\npackage 'mvtnorm' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\idrin\\AppData\\Local\\Temp\\Rtmp2HmPAQ\\downloaded_packages\n── R CMD build ─────────────────────────────────────────────────────────────────\n* checking for file 'C:\\Users\\idrin\\AppData\\Local\\Temp\\Rtmp2HmPAQ\\remotes6c3c17373177\\wilkelab-ungeviz-aeae12b/DESCRIPTION' ... OK\n* preparing 'ungeviz':\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\n* building 'ungeviz_0.1.0.tar.gz'\n\n\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#importing-the-data",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#importing-the-data",
    "title": "Hands-on Exercise 4 - Visualising Uncertainty",
    "section": "Importing the Data",
    "text": "Importing the Data\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package.\nreadr is a pacakge within tidyverse.\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\nexam_data tibble data frame contains:\n\nYear end examination grades of a cohort of primary 3 students from a local school.\nThere are a total of seven attributes. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "title": "Hands-on Exercise 4 - Visualising Uncertainty",
    "section": "Visualizing the uncertainty of point estimates: ggplot2 methods",
    "text": "Visualizing the uncertainty of point estimates: ggplot2 methods\nA point estimate is a single number, e.g., mean. Uncertainty, is expressed as standard error, confidence interval, or credible interval.\nThe code chunk below will be used to derive the necessary summary statistics.\n\ngroup_by() of dplyr package is used to group the observation by RACE,\nsummarise() is used to compute the count of observations, mean, standard deviation\nmutate() is used to derive standard error of Maths by RACE, and\nthe output is save as a tibble data table called my_sum.\n\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\nThe code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nTableCode\n\n\n\n\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n\n\nknitr::kable(head(my_sum), format = 'html')"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#plotting-standard-error-bars-of-point-estimates",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#plotting-standard-error-bars-of-point-estimates",
    "title": "Hands-on Exercise 4 - Visualising Uncertainty",
    "section": "Plotting standard error bars of point estimates",
    "text": "Plotting standard error bars of point estimates\nThe code chunk belows plots the standard error bars of mean maths score by race.\nNote:\n\nThe error bars are computed by using the formula mean+/-se.\nFor geom_point(), it is important to indicate stat=“identity”.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#plotting-confidence-interval-of-point-estimates",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#plotting-confidence-interval-of-point-estimates",
    "title": "Hands-on Exercise 4 - Visualising Uncertainty",
    "section": "Plotting confidence interval of point estimates",
    "text": "Plotting confidence interval of point estimates\nInstead of plotting the standard error bar of point estimates, the confidence intervals of mean maths score by race can also be plotted.\nNote:\n\nThe confidence intervals are computed by using the formula mean+/-1.96*se.\nThe error bars is sorted by using the average maths scores.\nlabs() argument of ggplot2 is used to change the x-axis label.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualizing-the-uncertainty-of-point-estimates-with-interactive-error-bars",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualizing-the-uncertainty-of-point-estimates-with-interactive-error-bars",
    "title": "Hands-on Exercise 4 - Visualising Uncertainty",
    "section": "Visualizing the uncertainty of point estimates with interactive error bars",
    "text": "Visualizing the uncertainty of point estimates with interactive error bars\nThe code chunk below plots interactive error bars for the 99% confidence interval of mean maths score by race.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualising-uncertainty-ggdist-package",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualising-uncertainty-ggdist-package",
    "title": "Hands-on Exercise 4 - Visualising Uncertainty",
    "section": "Visualising Uncertainty: ggdist package",
    "text": "Visualising Uncertainty: ggdist package\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist)."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualizing-the-uncertainty-of-point-estimates-ggdist-methods",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualizing-the-uncertainty-of-point-estimates-ggdist-methods",
    "title": "Hands-on Exercise 4 - Visualising Uncertainty",
    "section": "Visualizing the uncertainty of point estimates: ggdist methods",
    "text": "Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\nIn the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\nThe plot below shows 95% and 99% confidence intervals\n\nstat_pointinterval is used twice, once for each confidence interval.\nThe .width argument specifies the width of the intervals.\nThe .point argument specifies that we want to plot the median.\nThe .interval argument is set to “quantile” to indicate quantile-based intervals.\nscale_colour_manual is used to set custom colors for the confidence intervals and provide custom labels.\nOther aesthetic adjustments are made to improve the appearance of the plot, such as adjusting the size and position of the intervals.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(\n    .width = 0.95,\n    .point = \"median\",\n    .interval = \"quantile\",\n    aes(colour = \"95% CI\")) +\n  stat_pointinterval(\n    .width = 0.99,\n    .point = \"median\",\n    .interval = \"quantile\",\n    aes(colour = \"99% CI\")) +\n  scale_colour_manual(\n    values = c(\"95% CI\" = \"blue\", \"99% CI\" = \"red\"),\n    labels = c(\"95% CI\", \"99% CI\")) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\") +\n  theme_minimal()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualizing-the-uncertainty-of-point-estimates-ggdist-methods-1",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualizing-the-uncertainty-of-point-estimates-ggdist-methods-1",
    "title": "Hands-on Exercise 4 - Visualising Uncertainty",
    "section": "Visualizing the uncertainty of point estimates: ggdist methods",
    "text": "Visualizing the uncertainty of point estimates: ggdist methods\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualising-the-uncertainty-of-point-estimates-ggdist-methods",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualising-the-uncertainty-of-point-estimates-ggdist-methods",
    "title": "Hands-on Exercise 4 - Visualising Uncertainty",
    "section": "Visualising the uncertainty of point estimates: ggdist methods",
    "text": "Visualising the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Exercise 4 - Visualising Uncertainty",
    "section": "Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops-1",
    "href": "Hands_on_exercise/Hands_on_ex4/Hands-on_Ex04_2.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops-1",
    "title": "Hands-on Exercise 4 - Visualising Uncertainty",
    "section": "Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html",
    "href": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html",
    "title": "Hands-on Exercise 5 - Visualising and Analysing Text Data",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 5 - Visualising and Analysing Text Data",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\ntidytext, tidyverse (mainly readr, purrr, stringr, ggplot2)\nwidyr,\nwordcloud and ggwordcloud,\ntextplot (required igraph, tidygraph and ggraph, )\nDT,\nlubridate and hms.\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(tidytext, widyr, wordcloud, DT, ggwordcloud, textplot, lubridate, hms,tidyverse, tidygraph, ggraph, igraph)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html#importing-multiple-text-files-from-multiple-folders",
    "href": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html#importing-multiple-text-files-from-multiple-folders",
    "title": "Hands-on Exercise 5 - Visualising and Analysing Text Data",
    "section": "Importing Multiple Text Files from Multiple Folders",
    "text": "Importing Multiple Text Files from Multiple Folders\n\nCreating a folder list\n\nnews20 &lt;- \"data/news20/\"\n\n\n\nDefine a function to read all files from a folder into a data frame\n\nread_folder &lt;- function(infolder) {\n  tibble(file = dir(infolder, \n                    full.names = TRUE)) %&gt;%\n    mutate(text = map(file, \n                      read_lines)) %&gt;%\n    transmute(id = basename(file), \n              text) %&gt;%\n    unnest(text)\n}"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html#importing-multiple-text-files-from-multiple-folders-1",
    "href": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html#importing-multiple-text-files-from-multiple-folders-1",
    "title": "Hands-on Exercise 5 - Visualising and Analysing Text Data",
    "section": "Importing Multiple Text Files from Multiple Folders",
    "text": "Importing Multiple Text Files from Multiple Folders\n\nReading in all the messages from the 20news folder\n\nread_lines() of readr package is used to read up to n_max lines from a file.\nmap() of purrr package is used to transform their input by applying a function to each element of a list and returning an object of the same length as the input.\nunnest() of dplyr package is used to flatten a list-column of data frames back out into regular columns.\nmutate() of dplyr is used to add new variables and preserves existing ones;\ntransmute() of dplyr is used to add new variables and drops existing ones.\nread_rds() is used to save the extracted and combined data frame as rds file for future use.\n\n\nraw_text &lt;- tibble(folder = \n                     dir(news20, \n                         full.names = TRUE)) %&gt;%\n  mutate(folder_out = map(folder, \n                          read_folder)) %&gt;%\n  unnest(cols = c(folder_out)) %&gt;%\n  transmute(newsgroup = basename(folder), \n            id, text)\nwrite_rds(raw_text, \"data/rds/news20.rds\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html#initial-eda",
    "href": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html#initial-eda",
    "title": "Hands-on Exercise 5 - Visualising and Analysing Text Data",
    "section": "Initial EDA",
    "text": "Initial EDA\nFigure below shows the frequency of messages by newsgroup.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_text &lt;- read_rds(\"data/rds/news20.rds\")\nraw_text %&gt;%\n  group_by(newsgroup) %&gt;%\n  summarize(messages = n_distinct(id)) %&gt;%\n  ggplot(aes(messages, newsgroup)) +\n  geom_col(fill = \"lightblue\") +\n  labs(y = NULL)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html#introducing-tidytext",
    "href": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html#introducing-tidytext",
    "title": "Hands-on Exercise 5 - Visualising and Analysing Text Data",
    "section": "Introducing tidytext",
    "text": "Introducing tidytext\n\nUsing tidy data principles in processing, analysing and visualising text data.\nMuch of the infrastructure needed for text mining with tidy data frames already exists in packages like ‘dplyr’, ‘broom’, ‘tidyr’, and ‘ggplot2’.\n\n\nRemoving header and automated email signitures\nEach message contains certain structural elements and additional text that are undesirable for inclusion in the analysis. For example:\n\nHeader containing fields such as “from:” or “in_reply_to:”\nAutomated email signatures, which occur after a line like “–”.\n\nThe code chunk below uses:\n\ncumsum() of base R to return a vector whose elements are the cumulative sums of the elements of the argument.\nstr_detect() from stringr to detect the presence or absence of a pattern in a string.\n\n\ncleaned_text &lt;- raw_text %&gt;%\n  group_by(newsgroup, id) %&gt;%\n  filter(cumsum(text == \"\") &gt; 0,\n         cumsum(str_detect(\n           text, \"^--\")) == 0) %&gt;%\n  ungroup()\n\n\n\nRemoving lines with nested text representing quotes from other users\nRegular expressions are used to remove with nested text representing quotes from other users.\n\nstr_detect() from stringr is used to detect the presence or absence of a pattern in a string.\nfilter() of dplyr package is used to subset a data frame, retaining all rows that satisfy the specified conditions.\n\n\ncleaned_text &lt;- cleaned_text %&gt;%\n  filter(str_detect(text, \"^[^&gt;]+[A-Za-z\\\\d]\")\n         | text == \"\",\n         !str_detect(text, \n                     \"writes(:|\\\\.\\\\.\\\\.)$\"),\n         !str_detect(text, \n                     \"^In article &lt;\")\n  )\n\n\n\nText Data Processing\n\n unnest_tokens() of tidytext package is used to split the dataset into tokens\n stop_words() is used to remove stop-words\n\n\nusenet_words &lt;- cleaned_text %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word)\n\nHeaders, signatures and formatting have been removed. The code chunk below calculates individual word frequncies to explore common words in the dataset.\n\nusenet_words %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 5,542 × 2\n   word           n\n   &lt;chr&gt;      &lt;int&gt;\n 1 people        57\n 2 time          50\n 3 jesus         47\n 4 god           44\n 5 message       40\n 6 br            27\n 7 bible         23\n 8 drive         23\n 9 homosexual    23\n10 read          22\n# ℹ 5,532 more rows\n\n\nWord frequencies within newsgroup\n\nwords_by_newsgroup &lt;- usenet_words %&gt;%\n  count(newsgroup, word, sort = TRUE) %&gt;%\n  ungroup()\n\n\n\nVisualising Words in newsgroups\n\nwordcloud() of wordcloud package is used to plot a static wordcloud\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwordcloud(words_by_newsgroup$word,\n          words_by_newsgroup$n,\n          max.words = 300)\n\n\n\n\nA DT table can be used to complement the visual discovery.\n\nTableCode\n\n\n\n\n\n\n\n\n\n\n\n# Create a data frame with word frequency data\nword_freq_table &lt;- data.frame(Word = words_by_newsgroup$word,\n                              Frequency = words_by_newsgroup$n)\n\n# Render the DataTable\ndatatable(word_freq_table, \n          options = list(pageLength = 10))\n\n\n\n\n\n\nVisualising Words in newsgroups\n ggwordcloud package is used to plot the wordcloud below\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nwords_by_newsgroup %&gt;%\n  filter(n &gt; 0) %&gt;%\nggplot(aes(label = word,\n           size = n)) +\n  geom_text_wordcloud() +\n  theme_minimal() +\n  facet_wrap(~newsgroup)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html#basic-concept-of-tf-idf",
    "href": "Hands_on_exercise/Hands_on_ex5/Hands-on_Ex05.html#basic-concept-of-tf-idf",
    "title": "Hands-on Exercise 5 - Visualising and Analysing Text Data",
    "section": "Basic Concept of TF-IDF",
    "text": "Basic Concept of TF-IDF\ntf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection of corpus.\n\\(idf(term) = ln \\frac{n_{documents}}{n_{documents containing term}}\\)\n\nComputing tf-idf within newsgroups\nbind_tf_idf() of tidytext is used to compute and bind the term frequency, inverse document frequency and ti-idf of a tidy text dataset to the dataset.\n\ntf_idf &lt;- words_by_newsgroup %&gt;%\n  bind_tf_idf(word, newsgroup, n) %&gt;%\n  arrange(desc(tf_idf))\n\n\n\nVisualising tf-idf as interactive table\nInteractive table created by using datatable() to create a html table that allows pagination of rows and columns.\nThe code chunk below also uses:\n\nfilter() argument is used to turn control the filter UI.\nformatRound() is used to customise the values format. The argument digits define the number of decimal places.\nformatStyle() is used to customise the output table. In this example, the arguments target and lineHeight are used to reduce the line height by 25%.\n\n\nTableCode\n\n\n\n\n\n\n\n\n\n\n\nDT::datatable(tf_idf, filter = 'top') %&gt;% \n  formatRound(columns = c('tf', 'idf', \n                          'tf_idf'), \n              digits = 3) %&gt;%\n  formatStyle(0, \n              target = 'row', \n              lineHeight='25%')\n\n\n\n\n\n\nVisualising tf-idf within newsgroups\nFacet bar charts technique is used to visualise the tf-idf values of science related newsgroup.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntf_idf %&gt;%\n  filter(str_detect(newsgroup, \"^sci\\\\.\")) %&gt;%\n  group_by(newsgroup) %&gt;%\n  slice_max(tf_idf, \n            n = 12) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, \n                        tf_idf)) %&gt;%\n  ggplot(aes(tf_idf, \n             word, \n             fill = newsgroup)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ newsgroup, \n             scales = \"free\") +\n  labs(x = \"tf-idf\", \n       y = NULL)\n\n\n\n\n\n\nCounting and correlating pairs of words with the widyr package\n\nTo count the number of times that two words appear within the same document, or to see how correlated they are.\nMost operations for finding pairwise counts or correlations need to turn the data into a wide matrix first.\nwidyr package first ‘casts’ a tidy dataset into a wide matrix, performs an operation such as a correlation on it, then re-tidies the result.\n\nIn this code chunk below, pairwise_cor() of widyr package is used to compute the correlation between newsgroup based on the common words found.\n\nnewsgroup_cors &lt;- words_by_newsgroup %&gt;%\n  pairwise_cor(newsgroup, \n               word, \n               n, \n               sort = TRUE)\n\n\n\nVisualising correlation as a network\nRelationship between newgroups is visualised as a network graph\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(2017)\n\nnewsgroup_cors %&gt;%\n  filter(correlation &gt; .025) %&gt;%\n  graph_from_data_frame() %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(alpha = correlation, \n                     width = correlation)) +\n  geom_node_point(size = 6, \n                  color = \"lightblue\") +\n  geom_node_text(aes(label = name),\n                 color = \"red\",\n                 repel = TRUE) +\n  theme_void()\n\n\n\n\n\n\nBigram\nCreated by using unnest_tokens() of tidytext.\n\nBigramCode\n\n\n\n\n# A tibble: 28,827 × 3\n   newsgroup   id    bigram    \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;     \n 1 alt.atheism 54256 &lt;NA&gt;      \n 2 alt.atheism 54256 &lt;NA&gt;      \n 3 alt.atheism 54256 as i      \n 4 alt.atheism 54256 i don't   \n 5 alt.atheism 54256 don't know\n 6 alt.atheism 54256 know this \n 7 alt.atheism 54256 this book \n 8 alt.atheism 54256 book i    \n 9 alt.atheism 54256 i will    \n10 alt.atheism 54256 will use  \n# ℹ 28,817 more rows\n\n\n\n\n\nbigrams &lt;- cleaned_text %&gt;%\n  unnest_tokens(bigram, \n                text, \n                token = \"ngrams\", \n                n = 2)\n\nbigrams\n\n\n\n\n\n\nCounting bigrams\nCount and sort the bigram data frame ascendingly\n\nBigram CountCode\n\n\n\n\n# A tibble: 19,888 × 2\n   bigram       n\n   &lt;chr&gt;    &lt;int&gt;\n 1 of the     169\n 2 in the     113\n 3 to the      74\n 4 to be       59\n 5 for the     52\n 6 i have      48\n 7 that the    47\n 8 if you      40\n 9 on the      39\n10 it is       38\n# ℹ 19,878 more rows\n\n\n\n\n\nbigrams_count &lt;- bigrams %&gt;%\n  filter(bigram != 'NA') %&gt;%\n  count(bigram, sort = TRUE)\n\nbigrams_count\n\n\n\n\n\n\nCleaning bigram\nSeperate the bigram into two words\n\nBigramCode\n\n\n\n\n# A tibble: 4,607 × 4\n   newsgroup   id    word1        word2        \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        \n 1 alt.atheism 54256 defines      god          \n 2 alt.atheism 54256 term         preclues     \n 3 alt.atheism 54256 science      ideas        \n 4 alt.atheism 54256 ideas        drawn        \n 5 alt.atheism 54256 supernatural precludes    \n 6 alt.atheism 54256 scientific   assertions   \n 7 alt.atheism 54256 religious    dogma        \n 8 alt.atheism 54256 religion     involves     \n 9 alt.atheism 54256 involves     circumventing\n10 alt.atheism 54256 gain         absolute     \n# ℹ 4,597 more rows\n\n\n\n\n\nbigrams_separated &lt;- bigrams %&gt;%\n  filter(bigram != 'NA') %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), \n           sep = \" \")\n\nbigrams_filtered &lt;- bigrams_separated %&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\n\n\n\n\n\nCounting the bigram again\n\nbigram_counts &lt;- bigrams_filtered %&gt;% \n  count(word1, word2, sort = TRUE)\n\n\n\nCreate a network graph from bigram data frame\nA network graph is created by using graph_from_data_frame() of igraph package.\n\nbigram_graph &lt;- bigram_counts %&gt;%\n  filter(n &gt; 3) %&gt;%\n  graph_from_data_frame()\nbigram_graph\n\nIGRAPH fb49dd5 DN-- 40 24 -- \n+ attr: name (v/c), n (e/n)\n+ edges from fb49dd5 (vertex names):\n [1] 1          -&gt;2           1          -&gt;3           static     -&gt;void       \n [4] time       -&gt;pad         1          -&gt;4           infield    -&gt;fly        \n [7] mat        -&gt;28          vv         -&gt;vv          1          -&gt;5          \n[10] cock       -&gt;crow        noticeshell-&gt;widget      27         -&gt;1993       \n[13] 3          -&gt;4           child      -&gt;molestation cock       -&gt;crew       \n[16] gun        -&gt;violence    heat       -&gt;sink        homosexual -&gt;male       \n[19] homosexual -&gt;women       include    -&gt;xol         mary       -&gt;magdalene  \n[22] read       -&gt;write       rev        -&gt;20          tt         -&gt;ee         \n\n\n\n\nVisualizing a network of bigrams with ggraph\nggraph package is used to plot the bigram\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(1234)\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1)\n\n\n\n\n\n\nRevised version\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(1234)\n\na &lt;- grid::arrow(type = \"closed\", \n                 length = unit(.15,\n                               \"inches\"))\n\nggraph(bigram_graph, \n       layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), \n                 show.legend = FALSE,\n                 arrow = a, \n                 end_cap = circle(.07,\n                                  'inches')) +\n  geom_node_point(color = \"lightblue\", \n                  size = 5) +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1) +\n  theme_void()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex7/Hands-on_Ex07.html",
    "href": "Hands_on_exercise/Hands_on_ex7/Hands-on_Ex07.html",
    "title": "Hands-on Exercise 7 - Visualising and Analysing Time-Oriented Data",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex7/Hands-on_Ex07.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex7/Hands-on_Ex07.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 7 - Visualising and Analysing Time-Oriented Data",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\nscales\nviridis\nggthemes\ngridExtra\nreadxl\nknitr\ndata.table\ntidyverse\nlubridate\nCGPfunctions\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(scales, viridis, ggthemes, gridExtra, readxl, knitr, data.table, tidyverse, lubridate, CGPfunctions)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex7/Hands-on_Ex07.html#plotting-calendar-heatmap",
    "href": "Hands_on_exercise/Hands_on_ex7/Hands-on_Ex07.html#plotting-calendar-heatmap",
    "title": "Hands-on Exercise 7 - Visualising and Analysing Time-Oriented Data",
    "section": "Plotting Calendar Heatmap",
    "text": "Plotting Calendar Heatmap\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\nThe Data\nImport eventlog.csv into RStudio environment by using read_csv() of readr package.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\nExamining the data structure\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nattacks contains three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n\n\nData Preparation\nStep 1: Deriving weekday and hours of day fields\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\n\n\n\n\nNote\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\nNote\n\n\n\nmutate() of dplyr package is used to:\n\nextract necessary data into attacks dataframe\nconvert wkday and hour fields into factor so they’ll be ordered when plotting\n\n\n\nView dataframe\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\nBuilding the Calendar Heatmaps\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ngrouped tibble dataframe is derived by aggregating the attacks by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\nNext, group the count by hour and wkday, then plot it."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex7/Hands-on_Ex07.html#building-multiple-calendar-heatmaps",
    "href": "Hands_on_exercise/Hands_on_ex7/Hands-on_Ex07.html#building-multiple-calendar-heatmaps",
    "title": "Hands-on Exercise 7 - Visualising and Analysing Time-Oriented Data",
    "section": "Building Multiple Calendar Heatmaps",
    "text": "Building Multiple Calendar Heatmaps\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\n\nPlotting Multiple Calendar Heatmaps\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, the following steps need to be done:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nExtract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\nPlotting Multiple Calendar Heatmaps\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex7/Hands-on_Ex07.html#plotting-cycle-plot",
    "href": "Hands_on_exercise/Hands_on_ex7/Hands-on_Ex07.html#plotting-cycle-plot",
    "title": "Hands-on Exercise 7 - Visualising and Analysing Time-Oriented Data",
    "section": "Plotting Cycle Plot",
    "text": "Plotting Cycle Plot\nPlot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\nStep 1: Data Import\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\nStep 2: Deriving month and year fields\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\nStep 3: Extracting the target country\nThe code chunk below is use to extract data for the target country (i.e. Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\nStep 4: Computing year average arrivals by month\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\nStep 5: Plotting the cycle plot\nThe code chunk below is used to plot the cycle plot\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex7/Hands-on_Ex07.html#plotting-slopegraph",
    "href": "Hands_on_exercise/Hands_on_ex7/Hands-on_Ex07.html#plotting-slopegraph",
    "title": "Hands-on Exercise 7 - Visualising and Analysing Time-Oriented Data",
    "section": "Plotting Slopegraph",
    "text": "Plotting Slopegraph\nCGPfunctions will be used. Refer to Using newggslopegraph to learn more about the function. Read more about newggslopegraph() and its arguments by referring to its documentation.\n\nStep 1: Data Import\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\nStep 2: Plotting the slopegraph\nThe code chunk below will be used to plot a basic slopegraph.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "inclass/inclass01/inclass01.html",
    "href": "inclass/inclass01/inclass01.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "pacman::p_load(tidyverse)\n\n\nrealis &lt;- read_csv(\"data/realis2019.csv\")\n#prior setting of folders is important \n#if use read.csv then outcome wont be a tibl format, all the column names are changed"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Welcome to my ISSS608 Visual Analytic and Applications.\nReference :\nQuarto – HTML Theming\nReports & Presentations with Quarto\nQuarto Basics\nR for Visual Analytics"
  },
  {
    "objectID": "takehome/takehome2.html#step-1-load-library-and-data",
    "href": "takehome/takehome2.html#step-1-load-library-and-data",
    "title": "Take-home Exercise 2:Singapore Private Residential Market",
    "section": "Step 1: Load library and data",
    "text": "Step 1: Load library and data\n\n\nCode\n# Load library for data manipulation\n\npacman::p_load(ggplot2,plotly,dplyr,tidyverse,ggrepel)\n\n# Load in Data\n\nsetwd(\"C:/kekekay/ISSS608-VAA/takehome/data\")\nfull_data &lt;-  list.files(\n                    pattern = \"*.csv\",\n                    full.names=T) %&gt;%\n                    lapply(read_csv) %&gt;%\n                    bind_rows()"
  },
  {
    "objectID": "takehome/takehome2.html#step-2-aggregate-data-count-transactions-per-property-type",
    "href": "takehome/takehome2.html#step-2-aggregate-data-count-transactions-per-property-type",
    "title": "Take-home Exercise 2:Singapore Private Residential Market",
    "section": "Step 2: Aggregate Data: Count transactions per property type",
    "text": "Step 2: Aggregate Data: Count transactions per property type\n\n\nCode\ntransaction_counts &lt;- full_data %&gt;%\n  group_by(`Property Type`) %&gt;%\n  summarise(Transactions = n(), .groups = 'drop')  \n\n# percentages for labels\ntransaction_counts &lt;- transaction_counts %&gt;%\n  mutate(Percentage = Transactions / sum(Transactions) * 100)"
  },
  {
    "objectID": "takehome/takehome2.html#step-3-use-ggplot2-to-create-a-pie-chart-representing-the-number-of-transactions-for-each-property-type",
    "href": "takehome/takehome2.html#step-3-use-ggplot2-to-create-a-pie-chart-representing-the-number-of-transactions-for-each-property-type",
    "title": "Take-home Exercise 2:Singapore Private Residential Market",
    "section": "Step 3: Use ggplot2 to create a pie chart representing the number of transactions for each property type",
    "text": "Step 3: Use ggplot2 to create a pie chart representing the number of transactions for each property type\n\n\nCode\ncolors &lt;- c(\"#8dd3c7\", \"#ffffb3\", \"#bebada\", \"#fb8072\", \"#80b1d3\", \"#fdb462\", \"#b3de69\")\n\nggplot(transaction_counts, aes(x = \"\", y = Transactions, fill = `Property Type`)) +\n  geom_bar(stat = \"identity\", width = 4, color = \"white\") +\n  coord_polar(theta = \"y\") +\n  scale_fill_manual(values = colors) +\n  geom_label_repel(\n    aes(label = sprintf(\"%s (%.1f%%)\", `Property Type`, Percentage)),\n    nudge_x = 2 * cos(seq(0, 2 * pi, length.out = nrow(transaction_counts) + 1)[-nrow(transaction_counts) - 1]),  # Adjust for radial placement\n    nudge_y = 1 * sin(seq(0, 2 * pi, length.out = nrow(transaction_counts) + 1)[-nrow(transaction_counts) - 1]),  # Adjust for radial placement\n    arrow = arrow(length = unit(0.02, \"npc\"), type = \"closed\", ends = \"last\"),\n    size = 4,  # Adjust font size for readability\n    color = \"black\"\n  ) +\n  labs(title = \"No. of Transactions by Property Type\") +\n  theme_void() +\n  theme(legend.position = \"none\")  \n\n\n\n\n\n\n\n\n\nSince condominiums and apartments account for the highest number of transactions, we will conduct a detailed market analysis of their unit prices per square foot.\n\nCondominiumApartment\n\n\n\n\nCode\n# Filter data for only Condominiums\ncondo_data &lt;- full_data %&gt;%\n  filter(`Property Type` == \"Condominium\")\n\n# Sampling data\nsampled_data &lt;- condo_data[sample(nrow(condo_data), 500), ]\nsampled_data$`Transacted Price ($)` &lt;- sampled_data$`Transacted Price ($)` / 1000\n\n# Calculate Price per Sq ft\nsampled_data$`Price per Sq ft` &lt;- sampled_data$`Transacted Price ($)` / sampled_data$`Area (SQFT)`\n\n# Create the ggplot\nP3 &lt;- ggplot(data = sampled_data, aes(x = `Area (SQFT)`, y = `Transacted Price ($)`, size = `Price per Sq ft`, color = `Type of Sale`, \n                                      text = paste(\"Price: \", `Transacted Price ($)`, \"k&lt;br&gt;Area: \", `Area (SQFT)`, \n                                                   \"sqft&lt;br&gt;Type of Sale: \", `Type of Sale`, \n                                                   \"&lt;br&gt;Price per Sq ft: $\", round(`Price per Sq ft`, 2), \"/sqft\"))) +\n  geom_point(alpha = 0.3) +  # Increased transparency\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_size_area(max_size = 5) +  # Area scaled size for proportional visibility\n  scale_x_continuous(limits = c(200, 4000), breaks = seq(400, 4000, by = 200)) +\n  scale_y_continuous(limits = c(0, 10000)) +\n  labs(x = \"Condominium Size (sq ft)\", y = \"Sale Price (in $000)\",\n       title = \"Market Analysis: Unit Price(Sqft) for Condominiums\") +\n  guides(size = guide_legend(title = \"Price per Sq ft\")) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        panel.grid.major = element_line(color = \"gray\", size = 0.5),\n        panel.grid.minor = element_line(color = \"lightgray\", size = 0.25),\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Convert to interactive plot\nP3_interactive &lt;- ggplotly(P3, tooltip = \"text\")\n\n# Print the interactive plot\nP3_interactive\n\n\n\n\n\n\nThe plot legend indicates that resale properties generally have the lowest unit prices, as illustrated by the smaller size of the points. Resale transactions, which predominantly occur within the 400-1800 sq ft range, do not necessarily become more expensive with increased size, highlighting a non-linear pricing structure for larger properties.\nConversely, new sales show a clearer trend where larger units command higher prices per square foot, though the maximum size for new condominiums caps at 2400 sq ft, smaller compared to resales which extend up to 3600 sq ft.\nSubsales display relatively stable pricing, ranging from $1.62 to $2.25 per sq ft, which suggests a less volatile segment within the condominium market.\n\n\n\n\nCode\n# Filter data for only apartments\nap_data &lt;- full_data %&gt;% \n  filter(`Property Type` == \"Apartment\")\n\n# Sampling data\nsampled_data1 &lt;- ap_data[sample(nrow(ap_data), 500), ]\nsampled_data1$`Transacted Price ($)` &lt;- sampled_data1$`Transacted Price ($)` / 1000\n\n# Calculate Price per Sq ft\nsampled_data1$`Price per Sq ft` &lt;- sampled_data1$`Transacted Price ($)` / sampled_data1$`Area (SQFT)`\n\n# Create the ggplot\np2 &lt;- ggplot(data = sampled_data1, aes(x = `Area (SQFT)`, y = `Transacted Price ($)`, size = `Price per Sq ft`, color = `Type of Sale`, \n                                      text = paste(\"Price: \", `Transacted Price ($)`, \"k&lt;br&gt;Area: \", `Area (SQFT)`, \n                                                   \"sqft&lt;br&gt;Type of Sale: \", `Type of Sale`, \n                                                   \"&lt;br&gt;Price per Sq ft: $\", round(`Price per Sq ft`, 2), \"/sqft\"))) +\n  geom_point(alpha = 0.3) +  # Increased transparency\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_size_area(max_size = 5) +  # Area scaled size for proportional visibility\n  scale_x_continuous(limits = c(200, 4000), breaks = seq(400, 4000, by = 200)) +\n  scale_y_continuous(limits = c(0, 10000)) +\n  labs(x = \"Apartment Size (sq ft)\", y = \"Sale Price (in $000)\",\n       title = \"Market Analysis: Unit Price(Sqft) for Apartment\") +\n  guides(size = guide_legend(title = \"Price per Sq ft\")) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        panel.grid.major = element_line(color = \"gray\", size = 0.5),\n        panel.grid.minor = element_line(color = \"lightgray\", size = 0.25),\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Convert to interactive plot\np2_interactive &lt;- ggplotly(p2, tooltip = \"text\")\n\n# Print the interactive plot\np2_interactive\n\n\n\n\n\n\nWhen comparing apartments to condominiums, apartments tend to be smaller in size, which could explain their generally lower transaction prices.\nThe pricing trends for apartments largely mirror those observed in condominiums, with the exception of some notable differences in the resale market for larger units. Specifically, resale apartments larger than 2000 sq ft exhibit significant variability in unit price. A striking example is a 3100 sq ft unit priced at an unusually low $0.98 per sq ft, which stands out as a potential outlier. This outlier may warrant further investigation to determine underlying factors that contribute to such an anomalously low unit price, such as location disadvantages, property condition, or market anomalies at the time of sale.\n\n\n\n\n\n\n\n\n\n\nTips of Interactive Features:\n\n\n\n\nUse the “Autoscale” button to automatically adjust the plot scale to fit within the view. This ensures all data is visible after zooming in or out.\nClick on legend entries to toggle the visibility of data points for each type of sale.\nHover over any data point to see detailed information, such as the price, area, type of sale, and price per square foot. This provides immediate insights without additional data references.\nThe size of each point indicates the price per square foot; larger points denote higher prices, allowing for quick visual assessment of property values.\nMove your cursor along the x-axis to compare data points from different types of sales at the same condominium size. This hover comparison helps identify trends and outliers within specific size ranges."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html",
    "title": "Hands-on Exercise 8 Part II - Visualising Geospatial Point Data",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 8 Part II - Visualising Geospatial Point Data",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\nTidyverse:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\n\nsf for handling geospatial data\ntmap for plotting choropleth maps\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#the-data",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#the-data",
    "title": "Hands-on Exercise 8 Part II - Visualising Geospatial Point Data",
    "section": "The Data",
    "text": "The Data\nThe data set used is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#data-import-and-preparation",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#data-import-and-preparation",
    "title": "Hands-on Exercise 8 Part II - Visualising Geospatial Point Data",
    "section": "Data Import and Preparation",
    "text": "Data Import and Preparation\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\n\nExamining the data content\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that the sgpools data in tibble data frame and not the common R data frame."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "title": "Hands-on Exercise 8 Part II - Visualising Geospatial Point Data",
    "section": "Creating a sf data frame from an aspatial data frame",
    "text": "Creating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe coords argument requires the column name of the x-coordinates to be provided first, followed by the column name of the y-coordinates.\nThe crs argument requires the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System to be provided. Country epsg codes can be found at epsg.io.\n\n\n\nFigure below shows the data table of sgpools_sf. Notice that a new column called geometry has been added into the data frame.\n\nDisplay the basic information of the newly created sgpools_sf \n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe output shows that sgppols_sf is in point feature class. It’s epsg ID is 3414. The bbox provides information of the extend of the geospatial data."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#interactive-point-symbol-map",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#interactive-point-symbol-map",
    "title": "Hands-on Exercise 8 Part II - Visualising Geospatial Point Data",
    "section": "Interactive point symbol map",
    "text": "Interactive point symbol map\nThe code chunks below are used to create an interactive point symbol map.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#proportional-symbol-map",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#proportional-symbol-map",
    "title": "Hands-on Exercise 8 Part II - Visualising Geospatial Point Data",
    "section": "Proportional Symbol Map",
    "text": "Proportional Symbol Map\nTo draw a proportional symbol map, the numerical variable needs to be assigned to the size visual attribute. The code chunk below shows the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#colour-visual-attribute",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#colour-visual-attribute",
    "title": "Hands-on Exercise 8 Part II - Visualising Geospatial Point Data",
    "section": "Colour Visual Attribute",
    "text": "Colour Visual Attribute\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunk below, OUTLET_TYPE variable is used as the colour attribute variable.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#faceted-plots",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_2.html#faceted-plots",
    "title": "Hands-on Exercise 8 Part II - Visualising Geospatial Point Data",
    "section": "Faceted Plots",
    "text": "Faceted Plots\ntmap’s view mode also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\nSwitch tmap’s viewer back to plot mode\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_3.html",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_3.html",
    "title": "Hands-on Exercise 8 Part III - Analytical Mapping",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_3.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_3.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 8 Part III - Analytical Mapping",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\nTidyverse:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\n\nsf for handling geospatial data\ntmap for plotting choropleth maps\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\nImporting data\n A data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level. You can find the data set in the rds sub-direct of the hands-on data folder.\nThe code chunk below uses read_rds() function of readr package to import NGA_wp.rds into R as a tibble data frame called NGA_wp.\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_3.html#basic-choropleth-mapping",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_3.html#basic-choropleth-mapping",
    "title": "Hands-on Exercise 8 Part III - Analytical Mapping",
    "section": "Basic Choropleth Mapping",
    "text": "Basic Choropleth Mapping\n\nVisualising distribution of non-functional water point\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\np2 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_3.html#choropleth-map-for-rates",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_3.html#choropleth-map-for-rates",
    "title": "Hands-on Exercise 8 Part III - Analytical Mapping",
    "section": "Choropleth Map for Rates",
    "text": "Choropleth Map for Rates\nAs water points are not equally distributed in space, map rates are more important than the count. If the location of the water points are not considered, total water point size will be mapped instead of the topic of interest.\n\nDeriving Proportion of Functional Water Points and Non-Functional Water Points\nIn the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_3.html#plotting-map-of-rate",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_3.html#plotting-map-of-rate",
    "title": "Hands-on Exercise 8 Part III - Analytical Mapping",
    "section": "Plotting map of rate",
    "text": "Plotting map of rate\nChoropleth map showing the distribution of percentage functional water point by LGA\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_3.html#extreme-value-maps",
    "href": "Hands_on_exercise/Hands_on_ex8/Hands-on_Ex08_3.html#extreme-value-maps",
    "title": "Hands-on Exercise 8 Part III - Analytical Mapping",
    "section": "Extreme Value Maps",
    "text": "Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\nPercentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note: the begin and endpoint need to be included.\n\nData Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is also extracted. For mapping and spatial manipulation, many base R functions cannot deal with the geometry. e.g., quantile() gives an error. Thus, st_set_geomtry(NULL) is used to drop geomtry field.\n\n\n\n\nCreating the get.var function\nFirstly, an R function as is written to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\nA percentile mapping function\nNext, a percentile mapping function is written by using the code chunk below.\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\nTest drive the percentile mapping function\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdditional arguments e.g., title, legend positioning can be passed to customise various features of the map.\n\n\n\n\n\nBox map\nA box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence.\nWhen there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\nCreating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\nCreating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\nTest drive the newly created function\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\nBoxmap function\nThe code chunk below is an R function to create a box map. Arguments include:\n\nvnam: variable name (as character, in quotes)\ndf: simple features polygon layer\nlegtitle: legend title\nmtitle: map title\nmult: multiplier for IQR\nreturns: a tmap-element (plots a map)\n\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_1.html",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_1.html",
    "title": "Hands-on Exercise 9 Part I - Building Ternary Plot with R",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_1.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_1.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 9 Part I - Building Ternary Plot with R",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\nTidyverse:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nNote: Version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(plotly, ggtern, tidyverse)\n\npackage 'DEoptimR' successfully unpacked and MD5 sums checked\npackage 'tensorA' successfully unpacked and MD5 sums checked\npackage 'robustbase' successfully unpacked and MD5 sums checked\npackage 'bayesm' successfully unpacked and MD5 sums checked\npackage 'compositions' successfully unpacked and MD5 sums checked\npackage 'latex2exp' successfully unpacked and MD5 sums checked\npackage 'proto' successfully unpacked and MD5 sums checked\npackage 'hexbin' successfully unpacked and MD5 sums checked\npackage 'ggtern' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\idrin\\AppData\\Local\\Temp\\RtmpMx0Yjn\\downloaded_packages"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_1.html#importing-data-into-r",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_1.html#importing-data-into-r",
    "title": "Hands-on Exercise 9 Part I - Building Ternary Plot with R",
    "section": "Importing Data into R",
    "text": "Importing Data into R\n\nThe Data\nThe Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\nImporting Data\nThe code chunk below uses the read_csv() function of readr package to import respopagsex2000to2018_tidy.csv into R\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\nPreparing the Data\nUse the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_1.html#plotting-ternary-diagram-with-r",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_1.html#plotting-ternary-diagram-with-r",
    "title": "Hands-on Exercise 9 Part I - Building Ternary Plot with R",
    "section": "Plotting Ternary Diagram with R",
    "text": "Plotting Ternary Diagram with R\n\nPlotting a static ternary diagram\nUse ggtern() function of ggtern package to create a simple ternary plot.\nActive, Young, Old\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\nPopulation Structure 2015\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_1.html#plotting-an-interative-ternary-diagram",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_1.html#plotting-an-interative-ternary-diagram",
    "title": "Hands-on Exercise 9 Part I - Building Ternary Plot with R",
    "section": "Plotting an interative ternary diagram",
    "text": "Plotting an interative ternary diagram\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html",
    "title": "Hands-on Exercise 9 Part II - Visual Correlation Analysis",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 9 Part II - Visual Correlation Analysis",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\nTidyverse:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\n\nggstatplot\ncorrplot\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html#importing-data-into-r",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html#importing-data-into-r",
    "title": "Hands-on Exercise 9 Part II - Visual Correlation Analysis",
    "section": "Importing Data into R",
    "text": "Importing Data into R\n\nThe Data\nThe Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\n\nImporting Data\nThe code chunk below uses the read_csv() function of readr package to import the data into R\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nBesides quality and type, the rest of the variables are numerical and continuous data type.\n\n\n\n\nBuilding Correlation Matrix: pairs() method\nThere are more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html#building-a-basic-correlation-matrix",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html#building-a-basic-correlation-matrix",
    "title": "Hands-on Exercise 9 Part II - Visual Correlation Analysis",
    "section": "Building a basic correlation matrix",
    "text": "Building a basic correlation matrix\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npairs(wine[,1:11])\n\n\n\n\n\nDrawing the lower corner\npairs function of R Graphics provids many customisation arguments. e.g., it is common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\nSimilarly, you can display the upper half of the correlation matrix\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\nIncluding with correlation coefficients\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Exercise 9 Part II - Visual Correlation Analysis",
    "section": "Visualising Correlation Matrix: ggcormat()",
    "text": "Visualising Correlation Matrix: ggcormat()\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nAdditionally, some R packages e.g., ggstatsplot package also provides functions for building corrgrams.\n\nThe basic plot\nOne of the advantages of using ggcorrmat() to visualise a correlation matrix is its ability to provide a comprehensive and yet professional statistical report.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provides additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\n\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html#building-multiple-plots",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html#building-multiple-plots",
    "title": "Hands-on Exercise 9 Part II - Visual Correlation Analysis",
    "section": "Building multiple plots",
    "text": "Building multiple plots\nSince ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_2.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on Exercise 9 Part II - Visual Correlation Analysis",
    "section": "Visualising Correlation Matrix using corrplot Package",
    "text": "Visualising Correlation Matrix using corrplot Package\n\nGetting started with corrplot\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n\n\nWorking with visual geometrics\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\nWorking with layout\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\nThe default layout of the corrgram can be further customised. e.g., arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\nWorking with mixed layout\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n\n\nCombining corrgram with the significant test\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables..\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\nReorder a corrgram\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\nReordering a correlation matrix using hclust\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_3.html",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_3.html",
    "title": "Hands-on Exercise 9 Part III - Heatmap for Visualising and Analysing Multivariate Data",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_3.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_3.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 9 Part III - Heatmap for Visualising and Analysing Multivariate Data",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\nseriation\nheatmaply\ndendextend\ntidyverse\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\npackage 'iterators' successfully unpacked and MD5 sums checked\npackage 'permute' successfully unpacked and MD5 sums checked\npackage 'ca' successfully unpacked and MD5 sums checked\npackage 'foreach' successfully unpacked and MD5 sums checked\npackage 'gclus' successfully unpacked and MD5 sums checked\npackage 'qap' successfully unpacked and MD5 sums checked\npackage 'registry' successfully unpacked and MD5 sums checked\npackage 'TSP' successfully unpacked and MD5 sums checked\npackage 'vegan' successfully unpacked and MD5 sums checked\npackage 'seriation' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\idrin\\AppData\\Local\\Temp\\RtmpOwg9eB\\downloaded_packages\npackage 'assertthat' successfully unpacked and MD5 sums checked\npackage 'egg' successfully unpacked and MD5 sums checked\npackage 'heatmaply' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\idrin\\AppData\\Local\\Temp\\RtmpOwg9eB\\downloaded_packages"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_3.html#importing-data-into-r",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_3.html#importing-data-into-r",
    "title": "Hands-on Exercise 9 Part III - Heatmap for Visualising and Analysing Multivariate Data",
    "section": "Importing Data into R",
    "text": "Importing Data into R\n\nThe Data\nThe data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n\nImporting Data\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\n\nPreparing the data\nChange the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that the row number has been replaced into the country name."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_3.html#transforming-the-data-frame-into-a-matrix",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_3.html#transforming-the-data-frame-into-a-matrix",
    "title": "Hands-on Exercise 9 Part III - Heatmap for Visualising and Analysing Multivariate Data",
    "section": "Transforming the data frame into a matrix",
    "text": "Transforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make the heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that wh_matrix is in R matrix format."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_3.html#static-heatmap",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_3.html#static-heatmap",
    "title": "Hands-on Exercise 9 Part III - Heatmap for Visualising and Analysing Multivariate Data",
    "section": "Static Heatmap",
    "text": "Static Heatmap\nThere are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types.\n\n\nheatmap() of R Stats\nPlot a heatmap by using heatmap() of Base Stats.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\n\nTo plot a cluster heatmap, use the default\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe order of both rows and columns is different compared to the native wh_matrix. This is because heatmap does a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\n\nHere, red cells denote small values. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, this matrix needs to be normalised. This is done using the scale argument. It can be applied to rows or to columns.\nThe code chunk below normalises the matrix column-wise.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_3.html#creating-interactive-heatmap",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_3.html#creating-interactive-heatmap",
    "title": "Hands-on Exercise 9 Part III - Heatmap for Visualising and Analysing Multivariate Data",
    "section": "Creating Interactive Heatmap",
    "text": "Creating Interactive Heatmap\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\n\nWorking with heatmaply\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nheatmaply(mtcars)\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\n\nData Trasformation\nWhen analysing multivariate data sets, it is very common that the variables include values that reflect different types of measurements. In general, these variable values have their own range. In order to ensure that all the variables have comparable values, data transformation is commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilise.\n\nScaling method\n\nWhen all variables come from or are assumed to come from a normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all closer to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\nNormalising method\n\nWhen variables in the data come from different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\nPercentising method\n\nThis is similar to ranking the variables, but instead of keeping the rank values, they are divided by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\nClustering algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\nManual approach\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\nStatistical approach\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\nFigure above shows that k=3 (i.e. highlighted in red) is suitable.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\nSeriation\nOne of the problems with hierarchical clustering is that rows do not have a definite order, it merely constrains the space of possible orderings. For example, 3 itmes A, B, and C have three possible orderings: ABC, ACB and BAC. If clustering them gives ((A+B)+C) as a tree, it is clear that C can’t end up between A and B, but there is no information as to which way to flip the A+B cluster. It is unknown if the ABC ordering will lead to a clearer-looking heatmap compared to the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimise the Hamiltonian path length that is restricted by the dendrogram structure. This means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimised. This is related to a restricted version of the travelling salesman problem.\nOptimal Leaf Ordering (OLO) starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimise the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\nWorking with colour palettes\nThe default colour palette used by heatmaply is viridis. heatmaply users, however, other colour palettes can be used to to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\nThe finishing touch\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html",
    "title": "Hands-on Exercise 9 Part V - Treemap Visualisation",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 9 Part V - Treemap Visualisation",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\ntreemap\ntreemapify\ntidyverse\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#importing-data-into-r",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#importing-data-into-r",
    "title": "Hands-on Exercise 9 Part V - Treemap Visualisation",
    "section": "Importing Data into R",
    "text": "Importing Data into R\n\nThe Data\nREALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal of Urban Redevelopment Authority (URA).\n\n\nImporting Data\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data frame\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#data-wrangling-and-manipulation",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#data-wrangling-and-manipulation",
    "title": "Hands-on Exercise 9 Part V - Treemap Visualisation",
    "section": "Data Wrangling and Manipulation",
    "text": "Data Wrangling and Manipulation\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. The following steps need to be performed to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key functions of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows.\nGrouping effects are as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\ngroup_by() will used together with summarise() to derive the summarised data.frame.\n\nGrouped summaries without the Pipe\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\nNote\n\n\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, although they do not contribute to later analysis much.\n\n\n\n\nGrouped summaries with the pipe\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#designing-treemap-with-treemap-package",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#designing-treemap-with-treemap-package",
    "title": "Hands-on Exercise 9 Part V - Treemap Visualisation",
    "section": "Designing Treemap with treemap Package",
    "text": "Designing Treemap with treemap Package\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments.\n\nDesigning a static treemap\ntreemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\nUsing the basic arguments\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\nCode\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, i.e. median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, i.e., the hierarchy of planning areas.\n\n\n\n\nWorking with vColor and type arguments\nIn the code chunk below, type argument is defined as a value.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000."
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#colours-in-treemap-package",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#colours-in-treemap-package",
    "title": "Hands-on Exercise 9 Part V - Treemap Visualisation",
    "section": "Colours in treemap package",
    "text": "Colours in treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n16.4.5 The “value” type treemap\nThe code chunk below shows a value type treemap.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlthough the colour palette used is RdYlBu, there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why only 5000 to 45000 is shown in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\n\nThe “manual” type treemap\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\nCode\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe colour scheme used is very confusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\n\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\nTreemap Layout\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\nWorking with algorithm argument\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\nUsing sortID\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#designing-treemap-using-treemapify-package",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on Exercise 9 Part V - Treemap Visualisation",
    "section": "Designing Treemap using treemapify Package",
    "text": "Designing Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2.\n\nDesigning a basic treemap\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\nDefining hierarchy\nGroup by Planning Region\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\nGroup by Planning Area\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\nAdding boundary line\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_5.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on Exercise 9 Part V - Treemap Visualisation",
    "section": "Designing Interactive Treemap using d3treeR",
    "text": "Designing Interactive Treemap using d3treeR\n\nInstalling d3treeR package\ninstall devtools package \n\n#install.packages(\"devtools\")\n\nload the devtools library\n\nlibrary(devtools)\n\ninstall package found in github\n\n#install_github(\"timelyportfolio/d3treeR\")\n\nlaunch d3treeR package\n\nlibrary(d3treeR)\n\n\n\nDesigning An Interactive Treemap\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nUse d3tree() to build an interactive treemap\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_4.html",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_4.html",
    "title": "Hands-on Exercise 9 Part IV - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_4.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_4.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 9 Part IV - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\nGGally\nparallelPlot\ntidyverse\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_4.html#importing-data-into-r",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_4.html#importing-data-into-r",
    "title": "Hands-on Exercise 9 Part IV - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "Importing Data into R",
    "text": "Importing Data into R\n\nThe Data\nThe World Happinees 2018 data will be used. The data set is downloaded here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n\nImporting Data\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_4.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_4.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on Exercise 9 Part IV - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "Plotting Static Parallel Coordinates Plot",
    "text": "Plotting Static Parallel Coordinates Plot\nPlot static parallel coordinates plot by using ggparcoord() of GGally package.\n\nPlotting a simple parallel coordinates\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n\n\nPlotting a parallel coordinates with boxplot\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. Makeover the plot by using a collection of arguments provided by ggparcoord().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\n\nParallel coordinates with facet\nSince ggparcoord() is developed by extending ggplot2 package, ggplot2 functions can be combined with it when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n\n\nRotating x-axis text label\nFor ease of reading the x-axis text labels, rotate the labels by 30 degrees using theme() function in ggplot2.\n\nCode\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nTo rotate x-axis text labels, axis.text.x is used as an argument to the theme() function. element_text(angle = 30) rotates the x-axis text by an angle 30 degree.\n\n\n\n\n\nAdjusting the rotated x-axis text label\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot. This can be avoided by adjusting the text location using hjust argument to theme’s text element with element_text(). axis.text.x is used to change the look of x-axis text.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_4.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands_on_exercise/Hands_on_ex9/Hands-on_Ex09_4.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on Exercise 9 Part IV - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "Plotting Interactive Parallel Coordinates Plot: parallelPlot methods",
    "text": "Plotting Interactive Parallel Coordinates Plot: parallelPlot methods\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js.\n\nThe basic plot\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: Some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\n\n\nRotate axis label\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\nOne of the useful interactive feature of parallelPlot is that a variable of interest can be clicked, e.g., Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme.\n\n\nChanging the colour scheme\nChange the default blue colour scheme by using continousCS argument as shown in the code chunk below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\nParallel coordinates plot with histogram\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html",
    "href": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html",
    "title": "Hands-on Exercise 10 - Information Dashboard Design: R methods",
    "section": "",
    "text": "Learning Objectives:"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html#installing-and-loading-the-required-libraries",
    "href": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 10 - Information Dashboard Design: R methods",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nThe following R packages will be used:\n\ntidyverse provides a collection of functions for performing data science task such as importing, tidying, wrangling data and visualising data. It is not a single package but a collection of modern R packages including but not limited to readr, tidyr, dplyr, ggplot, tibble, stringr, forcats and purrr.\nlubridate provides functions to work with dates and times more efficiently.\nggthemes is an extension of ggplot2. It provides additional themes beyond the basic themes of ggplot2.\ngtExtras provides some additional helper functions to assist in creating beautiful tables with gt, an R package specially designed for anyone to make wonderful-looking tables using the R programming language.\nreactable provides functions to create interactive data tables for R, based on the React Table library and made with reactR.\nreactablefmtr provides various features to streamline and enhance the styling of interactive reactable tables with easy-to-use and highly-customizable functions and themes.\nsvglite\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\npacman::p_load(lubridate, ggthemes, reactable,\nreactablefmtr, gt, gtExtras, tidyverse,svglite)"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html#importing-microsoft-access-database",
    "href": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html#importing-microsoft-access-database",
    "title": "Hands-on Exercise 10 - Information Dashboard Design: R methods",
    "section": "Importing Microsoft Access database",
    "text": "Importing Microsoft Access database\n\nThe Data\n A personal database in Microsoft Access mdb format called Coffee Chain will be used.\n\n\nImporting Data\nodbcConnectAccess() of RODBC package is used used to import a database query table into R.\nIf an issue arises, change the R environment from 64bit to 32 bit:\nFrom R Studio &gt; Tools &gt; Global Options &gt; General &gt; Basic &gt; R session &gt; R version: Change &gt; Use your machine’s default version of R (32-bit).\nNote: Since R 4.2.0, 32-bit builds are no longer provided.\n\nlibrary(RODBC)\ncon &lt;- odbcConnectAccess2007('data/Coffee Chain.mdb')\ncoffeechain &lt;- sqlFetch(con, 'CoffeeChain Query')\nwrite_rds(coffeechain, \"data/CoffeeChain.rds\")\nodbcClose(con)\n\n\n\nPreparing the Data\nThe code chunk below is used to import CoffeeChain.rds into R.\n\ncoffeechain &lt;- read_rds(\"data/rds/CoffeeChain.rds\")\n\n\n\n\n\n\n\nNote\n\n\n\nNote: If coffeechain is already available in R, above step is optional\n\n\nThe code chunk below is used to aggregate Sales and Budgeted Sales at the Product level.\n\nproduct &lt;- coffeechain %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %&gt;%\n  ungroup()\n\n\n\nBullet chart in ggplot2\nThe code chunk below is used to plot the bullet charts using ggplot2 functions.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey60\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey50\", width=0.85) +\n  geom_col(aes(Product, current), \n           width=0.35,\n           fill = \"black\") + \n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"red\",\n                size = 1) +\n  coord_flip()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html#plotting-sparklines-using-ggplot2",
    "href": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html#plotting-sparklines-using-ggplot2",
    "title": "Hands-on Exercise 10 - Information Dashboard Design: R methods",
    "section": "Plotting sparklines using ggplot2",
    "text": "Plotting sparklines using ggplot2\n\nPreparing the data\n\nsales_report &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  group_by(Month, Product) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup() %&gt;%\n  select(Month, Product, Sales)\n\nCompute the minimum, maximum and end of the monthly sales\n\nmins &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.min(Sales))\nmaxs &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.max(Sales))\nends &lt;- group_by(sales_report, Product) %&gt;% \n  filter(Month == max(Month))\n\nCompute the 25 and 75 quantiles\n\nquarts &lt;- sales_report %&gt;%\n  group_by(Product) %&gt;%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %&gt;%\n  right_join(sales_report)\n\n\n\nsparklines in ggplot2\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html#static-information-dashboard-design-gt-and-gtextras-methods",
    "href": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html#static-information-dashboard-design-gt-and-gtextras-methods",
    "title": "Hands-on Exercise 10 - Information Dashboard Design: R methods",
    "section": "Static Information Dashboard Design: gt and gtExtras methods",
    "text": "Static Information Dashboard Design: gt and gtExtras methods\nCreate static information dashboard by using gt and gtExtras packages.\n\nPlotting a simple bullet chart\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\ncurrent\n\n\n\n\nAmaretto\n\n\n\n   \n\n\n\nCaffe Latte\n\n\n\n   \n\n\n\nCaffe Mocha\n\n\n\n   \n\n\n\nChamomile\n\n\n\n   \n\n\n\nColombian\n\n\n\n   \n\n\n\nDarjeeling\n\n\n\n   \n\n\n\nDecaf Espresso\n\n\n\n   \n\n\n\nDecaf Irish Cream\n\n\n\n   \n\n\n\nEarl Grey\n\n\n\n   \n\n\n\nGreen Tea\n\n\n\n   \n\n\n\nLemon\n\n\n\n   \n\n\n\nMint\n\n\n\n   \n\n\n\nRegular Espresso\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nproduct %&gt;%\n  gt::gt() %&gt;%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html#sparklines-gtextras-method",
    "href": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html#sparklines-gtextras-method",
    "title": "Hands-on Exercise 10 - Information Dashboard Design: R methods",
    "section": "sparklines: gtExtras method",
    "text": "sparklines: gtExtras method\n\nData Preparation\n\nreport &lt;- coffeechain %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  filter(Year == \"2013\") %&gt;%\n  mutate (Month = month(Date, \n                        label = TRUE, \n                        abbr = TRUE)) %&gt;%\n  group_by(Product, Month) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nImportant\n\n\n\nA requirement of gtExtras function: data.frame must be passed with list columns. Thus, the report data.frame must be converted into list columns.\n\n\nConvert Data Frame into List Columns\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# A tibble: 13 × 2\n   Product           `Monthly Sales`\n   &lt;chr&gt;             &lt;list&gt;         \n 1 Amaretto          &lt;dbl [12]&gt;     \n 2 Caffe Latte       &lt;dbl [12]&gt;     \n 3 Caffe Mocha       &lt;dbl [12]&gt;     \n 4 Chamomile         &lt;dbl [12]&gt;     \n 5 Colombian         &lt;dbl [12]&gt;     \n 6 Darjeeling        &lt;dbl [12]&gt;     \n 7 Decaf Espresso    &lt;dbl [12]&gt;     \n 8 Decaf Irish Cream &lt;dbl [12]&gt;     \n 9 Earl Grey         &lt;dbl [12]&gt;     \n10 Green Tea         &lt;dbl [12]&gt;     \n11 Lemon             &lt;dbl [12]&gt;     \n12 Mint              &lt;dbl [12]&gt;     \n13 Regular Espresso  &lt;dbl [12]&gt;     \n\n\n\n\nPlotting Coffechain Sales report\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\") %&gt;%\n   gt() %&gt;%\n   gt_plt_sparkline('Monthly Sales',\n                    same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMonthly Sales\n\n\n\n\nAmaretto\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n\n\n\n   3.7K\n\n\n\nChamomile\n\n\n\n   3.3K\n\n\n\nColombian\n\n\n\n   5.5K\n\n\n\nDarjeeling\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n\n\n\n   2.7K\n\n\n\nEarl Grey\n\n\n\n   3.0K\n\n\n\nGreen Tea\n\n\n\n   1.5K\n\n\n\nLemon\n\n\n\n   4.4K\n\n\n\nMint\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n\nAdding statistics\n\nreport %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            ) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = 4,\n    decimals = 2)\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\n\n\n\n\nAmaretto\n1016\n1210\n1,119.00\n\n\nCaffe Latte\n1398\n1653\n1,528.33\n\n\nCaffe Mocha\n3322\n3828\n3,613.92\n\n\nChamomile\n2967\n3395\n3,217.42\n\n\nColombian\n5132\n5961\n5,457.25\n\n\nDarjeeling\n2926\n3281\n3,112.67\n\n\nDecaf Espresso\n3181\n3493\n3,326.83\n\n\nDecaf Irish Cream\n2463\n2901\n2,648.25\n\n\nEarl Grey\n2730\n3005\n2,841.83\n\n\nGreen Tea\n1339\n1476\n1,398.75\n\n\nLemon\n3851\n4418\n4,080.83\n\n\nMint\n1388\n1669\n1,519.17\n\n\nRegular Espresso\n890\n1218\n1,023.42\n\n\n\n\n\n\n\n\n\n\nCombining the data.frame\nMonthly Sales\n\nspark &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\nSales statistics\n\nsales &lt;- report %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            )\n\nCombine all\n\nsales_data = left_join(sales, spark)\n\n\n\nPlotting the updated data.table\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales',\n                   same_limit = FALSE)\n\n\n\n\n\n\nCombining bullet chart and sparklines\nBullet Chart\n\nbullet &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`Target` = sum(`Budget Sales`),\n            `Actual` = sum(`Sales`)) %&gt;%\n  ungroup() \n\nJoin data\n\nsales_data = sales_data %&gt;%\n  left_join(bullet)\n\nPlot Data\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\nActual\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\n\n   \n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\n\n   \n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\n\n   \n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\n\n   \n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\n\n   \n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\n\n   \n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\n\n   \n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales') %&gt;%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 28,\n                palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()"
  },
  {
    "objectID": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-methods",
    "href": "Hands_on_exercise/Hands_on_ex10/Hands-on_Ex10.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-methods",
    "title": "Hands-on Exercise 10 - Information Dashboard Design: R methods",
    "section": "Interactive Information Dashboard Design: reactable and reactablefmtr methods",
    "text": "Interactive Information Dashboard Design: reactable and reactablefmtr methods\nCreate interactive information dashboard by using reactable and reactablefmtr packages.\nIn order to build an interactive sparklines, dataui R package needs to be installed.\n\nremotes::install_github(\"timelyportfolio/dataui\")\n\nrlang     (1.1.3  -&gt; 1.1.4 ) [CRAN]\nfastmap   (1.1.1  -&gt; 1.2.0 ) [CRAN]\ndigest    (0.6.35 -&gt; 0.6.36) [CRAN]\ncli       (3.6.2  -&gt; 3.6.3 ) [CRAN]\nhighr     (0.10   -&gt; 0.11  ) [CRAN]\ncachem    (1.0.8  -&gt; 1.1.0 ) [CRAN]\nxfun      (0.43   -&gt; 0.45  ) [CRAN]\ntinytex   (0.50   -&gt; 0.51  ) [CRAN]\nknitr     (1.46   -&gt; 1.47  ) [CRAN]\nevaluate  (0.23   -&gt; 0.24.0) [CRAN]\nrmarkdown (2.26   -&gt; 2.27  ) [CRAN]\nreactR    (0.5.0  -&gt; 0.6.0 ) [CRAN]\n\n  There is a binary version available but the source version is later:\n       binary source needs_compilation\nreactR  0.5.0  0.6.0             FALSE\n\npackage 'rlang' successfully unpacked and MD5 sums checked\npackage 'fastmap' successfully unpacked and MD5 sums checked\npackage 'digest' successfully unpacked and MD5 sums checked\npackage 'cli' successfully unpacked and MD5 sums checked\npackage 'highr' successfully unpacked and MD5 sums checked\npackage 'cachem' successfully unpacked and MD5 sums checked\npackage 'xfun' successfully unpacked and MD5 sums checked\npackage 'tinytex' successfully unpacked and MD5 sums checked\npackage 'knitr' successfully unpacked and MD5 sums checked\npackage 'evaluate' successfully unpacked and MD5 sums checked\npackage 'rmarkdown' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\idrin\\AppData\\Local\\Temp\\RtmpkfsYnK\\downloaded_packages\n── R CMD build ─────────────────────────────────────────────────────────────────\n* checking for file 'C:\\Users\\idrin\\AppData\\Local\\Temp\\RtmpkfsYnK\\remotes75086b2f5d0d\\timelyportfolio-dataui-39583c6/DESCRIPTION' ... OK\n* preparing 'dataui':\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\nOmitted 'LazyData' from DESCRIPTION\n* building 'dataui_0.0.1.tar.gz'\n\n\n\nLoad the package onto R environment\n\nlibrary(dataui)\n\n\nPlotting interactive sparklines\nPrepare the list field\n\nreport &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize(`Monthly Sales` = list(Sales))\n\nNext, react_sparkline will be used to plot the sparklines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nreactable(\n  report,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\nChanging the pagesize\nBy default the pagesize is 10. Argument defaultPageSize is used to change the default setting.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\nAdding points and labels\nhighlight_points argument is used to show the minimum and maximum values points and label argument is used to label first and last values.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        labels = c(\"first\", \"last\")\n        )\n    )\n  )\n)\n\n\n\n\n\n\nAdding reference line\nstatline argument is used to show the mean line.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        statline = \"mean\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\nAdding bandline\nbandline can be added by using the bandline argument.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        line_width = 1,\n        bandline = \"innerquartiles\",\n        bandline_color = \"green\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\nChanging from sparkline to sparkbar\nInstead of displaying the values as sparklines, we can display them as sparkbars.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkbar(\n        report,\n        highlight_bars = highlight_bars(\n          min = \"red\", max = \"blue\"),\n        bandline = \"innerquartiles\",\n        statline = \"mean\")\n    )\n  )\n)"
  }
]