{
  "hash": "b9a9280c6a4f729d4df38792db9ba699",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"In-Class Exercise 5\"\nauthor: \"Ke Ke\"\ndate: \"11 May, 2024\"\ndate-modified: last-modified\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  freeze: true\n---\n\n\n# Getting Started\n\n## Installing and loading the required libraries\n\nThe following R packages will be used:\n\n-   tidytext\n\n-   tidyverse\n\n-   readtext\n\n-   quanteda\n\nCode chunk below will be used to check if these packages have been installed and also will load them into the working R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidytext, readtext, quanteda, tidytext, tidyverse)\n```\n:::\n\n\n## **Importing Multiple Text Files from Multiple Folders**\n\n### **Creating a folder list**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_folder <- \"data/articles/\"\n```\n:::\n\n\n### **Define a function to read all files from a folder into a data frame**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data <- readtext(paste0(\"data/articles\",\"/*\"))\n# alternate version: text_data <- readtext(\"data/articles/*\")\n```\n:::\n\n\nCheck dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_text <- corpus(text_data)\nsummary(corpus_text, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 338 documents, showing 5 documents:\n\n                                   Text Types Tokens Sentences\n Alvarez PLC__0__0__Haacklee Herald.txt   206    433        18\n    Alvarez PLC__0__0__Lomark Daily.txt   102    170        12\n   Alvarez PLC__0__0__The News Buoy.txt    90    200         9\n Alvarez PLC__0__1__Haacklee Herald.txt    96    187         8\n    Alvarez PLC__0__1__Lomark Daily.txt   241    504        21\n```\n\n\n:::\n:::\n\n\n### **Text Data Processing**\n\n-    [`unnest_tokens()`](https://www.rdocumentation.org/packages/tidytext/versions/0.3.1/topics/unnest_tokens) of **tidytext** package is used to split the dataset into tokens\n\n-    [`stop_words()`](https://rdrr.io/cran/tidytext/man/stop_words.html) is used to remove stop-words\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nreadtext object consisting of 338 documents and 0 docvars.\n# A data frame: 338 × 2\n  doc_id                                 text               \n  <chr>                                  <chr>              \n1 Alvarez PLC__0__0__Haacklee Herald.txt \"\\\"Marine San\\\"...\"\n2 Alvarez PLC__0__0__Lomark Daily.txt    \"\\\"Alvarez PL\\\"...\"\n3 Alvarez PLC__0__0__The News Buoy.txt   \"\\\"Alvarez PL\\\"...\"\n4 Alvarez PLC__0__1__Haacklee Herald.txt \"\\\"Alvarez PL\\\"...\"\n5 Alvarez PLC__0__1__Lomark Daily.txt    \"\\\"Alvarez PL\\\"...\"\n6 Alvarez PLC__0__1__The News Buoy.txt   \"\\\"Alvarez PL\\\"...\"\n# ℹ 332 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tokenize the text column\nunnest_words <- text_data %>%   \n  unnest_tokens(output = word, input = text) \n\n# Filter tokens to include only alphabetic characters and remove stop words\nunnest_words <- filter(unnest_words, \n                       str_detect(word, \"[a-z']$\"),          \n                       !word %in% stop_words$word)\n```\n:::\n\n\nView dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(unnest_words)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nreadtext object consisting of 6 documents and 0 docvars.\n# A data frame: 6 × 3\n  doc_id                                 word      text     \n* <chr>                                  <chr>     <chr>    \n1 Alvarez PLC__0__0__Haacklee Herald.txt marine    \"\\\"\\\"...\"\n2 Alvarez PLC__0__0__Haacklee Herald.txt sanctuary \"\\\"\\\"...\"\n3 Alvarez PLC__0__0__Haacklee Herald.txt aid       \"\\\"\\\"...\"\n4 Alvarez PLC__0__0__Haacklee Herald.txt boosts    \"\\\"\\\"...\"\n5 Alvarez PLC__0__0__Haacklee Herald.txt alvarez   \"\\\"\\\"...\"\n6 Alvarez PLC__0__0__Haacklee Herald.txt plc's     \"\\\"\\\"...\"\n```\n\n\n:::\n:::\n\n\nThe code chunk below calculates individual word frequencies to explore common words in the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunnest_words %>%\n  count(word,sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nreadtext object consisting of 3260 documents and 0 docvars.\n# A data frame: 3,260 × 3\n  word             n text     \n  <chr>        <int> <chr>    \n1 fishing       2177 \"\\\"\\\"...\"\n2 sustainable   1525 \"\\\"\\\"...\"\n3 company       1036 \"\\\"\\\"...\"\n4 practices      838 \"\\\"\\\"...\"\n5 industry       715 \"\\\"\\\"...\"\n6 transactions   696 \"\\\"\\\"...\"\n# ℹ 3,254 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data_splitted <- text_data %>%\n  separate_wider_delim(\"doc_id\",\n                       delim = \"__0__\",\n                       names = c(\"X\",\"Y\"),\n                       too_few = \"align_end\")\n```\n:::\n\n\n### **JSONLite**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(jsonlite, tidyverse)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmc1_data <- fromJSON(\"data/mc1.json\")\nmc2_data <- fromJSON(\"data/mc2.json\")\n```\n:::\n\n\nThe mc3.json file shows an error message indicating that there's an invalid character in the JSON text, specifically \"NaN\". As \"NaN\" is not recognised as a valid value, preprocessing of the JSON file to replace \"NaN\" is required.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read the JSON file as text\njson_text <- readLines(\"data/mc3.json\")\n\n# Replace \"NaN\" with \"null\"\njson_text_fixed <- gsub(\"NaN\", \"null\", json_text)\n\n# Write the fixed JSON text back to a file\nwriteLines(json_text_fixed, \"data/mc3_fixed.json\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmc3_data <- fromJSON(\"data/mc3_fixed.json\")\n```\n:::\n\n\n## To view the dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nview(mc1_data[[\"nodes\"]])\nview(mc1_data[[\"links\"]])\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}